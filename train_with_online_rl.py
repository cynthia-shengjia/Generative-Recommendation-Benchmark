import os
from torch.utils.data import DataLoader
from datetime import datetime
from accelerate import Accelerator
import hydra
from omegaconf import DictConfig, OmegaConf


from genrec.quantization.pipelines.rqvae_pipeline import RQVAETrainingPipeline
from genrec.quantization.tokenizers.rqvae_tokenizer import RQVAETokenizer
from genrec.data.datasets.generative.tiger_dataset import TigerDataset
from genrec.data.collators.generative.tiger_collator import TigerDataCollator
from genrec.utils.nni_utils import get_nni_params, update_config_with_nni
from genrec.utils.common_utils import set_seed
from genrec.utils.logging_utils import setup_logging
from genrec.utils.evaluation_utils import evaluate_model_with_constrained_beam_search
from genrec.utils.models_setup.conditional_t5_setup import create_t5_model
from genrec.utils.trainer_setup.online_rl.grpo_setup import setup_training


os.environ["TOKENIZERS_PARALLELISM"] = "false"


def setup_output_directories(base_output_dir: str = "./output"):
    """è®¾ç½®è¾“å‡ºç›®å½•ç»“æ„"""
    if "NNI_PLATFORM" in os.environ:
        nni_output_dir = os.environ["NNI_OUTPUT_DIR"]
        dirs = {
            'base': base_output_dir,
            'tokenizer': os.path.join(base_output_dir, 'tokenizer_model'),
            'model': os.path.join(base_output_dir, 'generation_model'),
            'checkpoints': os.path.join(base_output_dir, 'checkpoints'),
            'logs': os.path.join(nni_output_dir, 'logs')
        } 
    else:
        dirs = {
            'base': base_output_dir,
            'tokenizer': os.path.join(base_output_dir, 'tokenizer_model'),
            'model': os.path.join(base_output_dir, 'generation_model'),
            'checkpoints': os.path.join(base_output_dir, 'checkpoints'),
            'logs': os.path.join(base_output_dir, 'logs')
        }
    
    for dir_path in dirs.values():
        os.makedirs(dir_path, exist_ok=True)
    
    return dirs

def stage1_train_tokenizer(rqvae_config: dict, output_dirs: dict, force_retrain: bool = False):
    """é˜¶æ®µ1: è®­ç»ƒRQ-VAE tokenizer"""
    print("\n" + "="*60)
    print("é˜¶æ®µ1: è®­ç»ƒRQ-VAE Tokenizer")
    print("="*60)
    
    tokenizer_checkpoint = rqvae_config['checkpoint_path']
    item2tokens_path = rqvae_config['save_path']
    
    if not force_retrain and os.path.exists(tokenizer_checkpoint) and os.path.exists(item2tokens_path):
        print(f"å‘ç°å·²å­˜åœ¨çš„tokenizeræ£€æŸ¥ç‚¹: {tokenizer_checkpoint}")
        print("è·³è¿‡tokenizerè®­ç»ƒé˜¶æ®µ...")
        return True
    
    required_files = [rqvae_config['data_text_files'], rqvae_config['interaction_files']]
    for file_path in required_files:
        if not os.path.exists(file_path):
            print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
    
    try:
        pipeline = RQVAETrainingPipeline(rqvae_config)
        pipeline.run()
        print("RQ-VAE tokenizerè®­ç»ƒå®Œæˆ!")
        return True
    except Exception as e:
        print(f"RQ-VAE tokenizerè®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def stage2_train_generation_model(    
    model_config,     
    rqvae_config,     
    online_rl_config,
    output_dirs,     
    accelerator,     
    logger,     
    force_retrain=False    
):
    """é˜¶æ®µ2: è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼ˆä½¿ç”¨çº¦æŸbeam searchè¿›è¡Œè¯„ä¼°ï¼‰"""
    if accelerator.is_main_process:
        logger.info("\n" + "="*60)
        logger.info("é˜¶æ®µ2: è®­ç»ƒç”Ÿæˆæ¨¡å‹ (Post-training)")
        logger.info("="*60)
    
    # ============ ç¡®å®šæ¨¡å‹ä¿å­˜è·¯å¾„ ============
    # ä¼˜å…ˆä½¿ç”¨ online_rl_config ä¸­æŒ‡å®šçš„è·¯å¾„
    custom_save_path = online_rl_config.get('save_model_path', None)
    
    if custom_save_path:
        # ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜è·¯å¾„
        model_save_dir = custom_save_path
        os.makedirs(model_save_dir, exist_ok=True)
        model_save_path = os.path.join(model_save_dir, f"{model_config['dataset_name']}_final_model.pt")
        if accelerator.is_main_process:
            logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜è·¯å¾„: {model_save_dir}")
    else:
        # ä½¿ç”¨é»˜è®¤è·¯å¾„
        model_save_dir = output_dirs['model']
        model_save_path = model_config['model_save_path']
        if accelerator.is_main_process:
            logger.info(f"ä½¿ç”¨é»˜è®¤ä¿å­˜è·¯å¾„: {model_save_dir}")
    
    if not force_retrain and os.path.exists(model_save_path):
        if accelerator.is_main_process:
            logger.info(f"å‘ç°å·²å­˜åœ¨çš„æ¨¡å‹: {model_save_path}")
            logger.info("è·³è¿‡æ¨¡å‹è®­ç»ƒé˜¶æ®µ...")
        return True
        
    tokenizer_items2tokens_path = os.path.join(output_dirs['tokenizer'], 'item2tokens.json')
    if not os.path.exists(tokenizer_items2tokens_path):
        if accelerator.is_main_process:
            logger.info(f"é”™è¯¯: tokenizeræœªå®Œæˆè®­ç»ƒï¼Œæ‰¾ä¸åˆ°æ–‡ä»¶: {tokenizer_items2tokens_path}")
        return False
      
    tokenizer_object_path = rqvae_config['tokenizer_path']
    if not os.path.exists(tokenizer_object_path):
        if accelerator.is_main_process:
            logger.info(f"é”™è¯¯: æ‰¾ä¸åˆ°å®Œæ•´çš„tokenizerå¯¹è±¡æ–‡ä»¶: {tokenizer_object_path}")
            logger.info("è¯·å…ˆè¿è¡Œé˜¶æ®µ1è¿›è¡Œè®­ç»ƒã€‚")
        return False
        
    try:
        # ============ åŠ è½½ Tokenizer ============
        if accelerator.is_main_process:
            logger.info(f"æ­£åœ¨ä» {tokenizer_object_path} åŠ è½½å®Œæ•´çš„tokenizer...")
        tokenizer = RQVAETokenizer.load(tokenizer_object_path)
        if accelerator.is_main_process:
            logger.info(f"æˆåŠŸåŠ è½½tokenizerï¼ŒåŒ…å« {len(tokenizer.item2tokens)} ä¸ªç‰©å“çš„tokenæ˜ å°„")
            logger.info(f"Tokenizerçš„å®Œæ•´è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
              
        # ============ åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹ ============
        pretrained_model_path = online_rl_config.get('pretrained_model', None)
          
        if pretrained_model_path and os.path.exists(pretrained_model_path):
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (Hugging Face æ ¼å¼)
            if accelerator.is_main_process:
                logger.info(f"ğŸ”¥ ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½: {pretrained_model_path}")
              
            from transformers import T5ForConditionalGeneration
            model = T5ForConditionalGeneration.from_pretrained(pretrained_model_path)
              
            if accelerator.is_main_process:
                logger.info("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
        else:
            # ä»å¤´åˆ›å»ºæ–°æ¨¡å‹
            if accelerator.is_main_process:
                if pretrained_model_path:
                    logger.warning(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {pretrained_model_path}")
                logger.info("ä»å¤´å¼€å§‹åˆ›å»ºæ–°æ¨¡å‹...")
              
            model = create_t5_model(
                vocab_size=tokenizer.vocab_size,
                model_config=model_config
            )

        # ============ æ‰“å°æ¨¡å‹ä¿¡æ¯ ============
        if accelerator.is_main_process:
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            logger.info(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")
            logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
            logger.info("åˆ›å»ºæ•°æ®é›†...")

        # ============ åˆ›å»ºæ•°æ®é›† ============
        train_dataset = TigerDataset(
            data_interaction_files=model_config['data_interaction_files'],
            data_text_files=model_config['data_text_files'],
            tokenizer=tokenizer, 
            config=model_config, 
            mode='train',
        )
          
        valid_dataset = TigerDataset(
            data_interaction_files=model_config['data_interaction_files'],
            data_text_files=model_config['data_text_files'],
            tokenizer=tokenizer, 
            config=model_config, 
            mode='valid',
        )
          
        test_dataset = TigerDataset(
            data_interaction_files=model_config['data_interaction_files'],
            data_text_files=model_config['data_text_files'],
            tokenizer=tokenizer, 
            config=model_config, 
            mode='test',
        )

        # ============ åˆ›å»º Data Collator ============
        eval_data_collator = TigerDataCollator(
            max_seq_len=train_dataset.max_token_len,
            pad_token_id=tokenizer.pad_token,
            eos_token_id=tokenizer.eos_token,
            mode="train"
        )
          
        test_data_collator = TigerDataCollator(
            max_seq_len=train_dataset.max_token_len,
            pad_token_id=tokenizer.pad_token,
            eos_token_id=tokenizer.eos_token,
            mode='test'
        )
     
        test_dataloader = DataLoader(
            test_dataset, 
            batch_size=model_config['test_batch_size'],
            shuffle=False,
            collate_fn=test_data_collator
        )
          
        # ä½¿ç”¨ accelerator å‡†å¤‡æ•°æ®åŠ è½½å™¨
        test_dataloader = accelerator.prepare(test_dataloader)
          
        # ============ è®¡ç®— Batch Size ============
        train_batch_size = model_config['batch_size']
        test_batch_size = model_config['test_batch_size']
        num_devices = accelerator.num_processes
          
        if train_batch_size % num_devices != 0 or test_batch_size % num_devices != 0:
            if accelerator.is_main_process:
                logger.error(f"é”™è¯¯: è®­ç»ƒæ‰¹æ¬¡å¤§å° {train_batch_size} æˆ–æµ‹è¯•æ‰¹æ¬¡å¤§å° {test_batch_size} ä¸èƒ½è¢«è®¾å¤‡æ•°é‡ {num_devices} æ•´é™¤ã€‚")
            return False
          
        per_device_train_batch_size = train_batch_size // num_devices
        per_device_eval_batch_size = test_batch_size // num_devices
          
        if accelerator.is_main_process:
            logger.info(f"Batch Size é…ç½® (æ€»å…± {num_devices} ä¸ªè®¾å¤‡)")
            logger.info(f"  - è®­ç»ƒ: å…¨å±€ {train_batch_size} -> å•è®¾å¤‡ {per_device_train_batch_size}")
            logger.info(f"  - è¯„ä¼°: å…¨å±€ {test_batch_size} -> å•è®¾å¤‡ {per_device_eval_batch_size}")
          
        # ============ è®¾ç½®è®­ç»ƒå™¨ ============
        # å¦‚æœä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼Œéœ€è¦æ›´æ–° checkpoint_dir
        if custom_save_path:
            checkpoint_dir = os.path.join(model_save_dir, 'checkpoints')
            os.makedirs(checkpoint_dir, exist_ok=True)
            model_config['checkpoint_dir'] = checkpoint_dir
        
        trainer = setup_training(
            model,
            tokenizer,
            train_dataset,
            valid_dataset,
            model_config,
            online_rl_config,
            output_dirs,
            logger,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_eval_batch_size,
            train_data_collator=eval_data_collator
        )
        
        # ============ å¼€å§‹è®­ç»ƒ ============
        trainer.train()
        accelerator.wait_for_everyone()
          
        # ============ æµ‹è¯•è¯„ä¼° ============
        if accelerator.is_main_process:
            logger.info("ä½¿ç”¨çº¦æŸbeam searchè¿›è¡Œæµ‹è¯•è¯„ä¼°...")
          
        evaluate_model_with_constrained_beam_search(
            model=model,
            eval_dataloader=test_dataloader,
            accelerator=accelerator,
            tokenizer=tokenizer,
            k_list=model_config.get("k_list", [5, 10, 20]),
            num_beams=model_config.get("num_beams", 10),
            max_gen_length=model_config.get("max_gen_length", 5),
            logger=logger,
            mode="Test"
        )
          
        # ============ ä¿å­˜æœ€ç»ˆæ¨¡å‹ ============
        if "NNI_PLATFORM" not in os.environ:
            if accelerator.is_main_process:
                logger.info(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {model_save_dir}")
            
            # ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
            trainer.save_model(model_save_dir)
            
            if accelerator.is_main_process:
                logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_dir}")
                logger.info(f"   - Hugging Face æ ¼å¼æ–‡ä»¶")
                logger.info(f"   - æ£€æŸ¥ç‚¹ç›®å½•: {model_config.get('checkpoint_dir', 'N/A')}")
          
        if accelerator.is_main_process:
            logger.info("ç”Ÿæˆæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
          
        return True
          
    except Exception as e:
        if accelerator.is_main_process:
            logger.error(f"ç”Ÿæˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        return False

@hydra.main(version_base=None, config_path="config", config_name="online_rl")
def main(cfg: DictConfig):
    """ä¸»å‡½æ•°"""
    seed = getattr(cfg, 'seed', 42) 
    set_seed(seed)

    if "NNI_PLATFORM" in os.environ:
        nni_params = get_nni_params()
        cfg = update_config_with_nni(cfg, nni_params)

    accelerator = Accelerator(mixed_precision='no')
    device = accelerator.device
    # è®¾ç½®CUDAè®¾å¤‡
    logger = None
    
    output_dirs = setup_output_directories(cfg.output_dir)
    if accelerator.is_main_process:
        logger = setup_logging(output_dirs['logs'])
        logger.info(f"è¾“å‡ºç›®å½•å·²è®¾ç½®: {output_dirs['base']}")
        logger.info(f"æ•°æ®é›†: {cfg.dataset}")
        logger.info(f"è¾“å‡ºç›®å½•: {cfg.output_dir}")
        logger.info(f"æ£€æµ‹åˆ° {accelerator.num_processes} ä¸ªè¿›ç¨‹")
        logger.info(f"å½“å‰è¿›ç¨‹è¿è¡Œè®¾å¤‡: {device}")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨: {output_dirs['logs']}")
    
    success = True
    
    # è·å–RQ-VAEé…ç½®
    rqvae_config = OmegaConf.to_container(cfg.tokenizer, resolve=True)
    rqvae_config['device'] = device
    rqvae_config['tokenizer_path'] = os.path.join(output_dirs['tokenizer'], 'tokenizer.pkl')
    rqvae_config['save_path'] = os.path.join(output_dirs['tokenizer'], 'item2tokens.json')
    rqvae_config['checkpoint_path'] = os.path.join(output_dirs['tokenizer'], 'tokenizer_checkpoint.pth')
    
    if not cfg.skip_tokenizer:
        if accelerator.is_main_process:
            tokenizer_success = stage1_train_tokenizer(
                rqvae_config, output_dirs, force_retrain=cfg.force_retrain_tokenizer
            )
            if not tokenizer_success:
                logger.info("Tokenizerè®­ç»ƒå¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                return
            success = success and tokenizer_success
        accelerator.wait_for_everyone() # ç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆtokenizerè®­ç»ƒ
    elif accelerator.is_main_process:
        logger.info("è·³è¿‡tokenizerè®­ç»ƒé˜¶æ®µ")
    
    if not cfg.skip_model and success:
        # è·å–æ¨¡å‹é…ç½®
        model_config = OmegaConf.to_container(cfg.model, resolve=True)
        model_config['device'] = device
        model_config['dataset_name'] = cfg.dataset
        model_config['model_save_path'] = os.path.join(output_dirs['model'], f"{cfg.dataset}_final_model.pt")
        model_config['checkpoint_dir'] = output_dirs['checkpoints']

        online_rl_config = OmegaConf.to_container(cfg.online_rl, resolve=True)
        
        model_success = stage2_train_generation_model(
            model_config, 
            rqvae_config, 
            online_rl_config,
            output_dirs, 
            accelerator,
            force_retrain=cfg.force_retrain_model,
            logger=logger
        )
        success = success and model_success
    elif cfg.skip_model and accelerator.is_main_process:
        logger.info("è·³è¿‡ç”Ÿæˆæ¨¡å‹è®­ç»ƒé˜¶æ®µ")
    
    if accelerator.is_main_process:
        logger.info("\n" + "="*60)
        if success:
            logger.info("è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆ!")
            logger.info(f"æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ä¿å­˜åœ¨: {output_dirs['base']}")
        else:
            logger.info("è®­ç»ƒæµç¨‹ä¸­é‡åˆ°é”™è¯¯")
        logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*60)
    accelerator.wait_for_everyone()


if __name__ == '__main__':
    main()