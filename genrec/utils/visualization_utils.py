import os
import torch
import torch.optim as optim
from tqdm import tqdm
from datetime import datetime
from accelerate import Accelerator
from transformers import get_linear_schedule_with_warmup
from genrec.tokenizers.TigerTokenizer import TigerTokenizer
from tools.utils import calc_ndcg, tokens_to_item_id
from typing import Optional, Dict, Any, List, Union, Callable
import math
import logging
import re
from transformers import T5ForConditionalGeneration,T5Config,Trainer
import numpy as np
import math
from transformers import EvalPrediction
from functools import partial
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback
from transformers import TrainerCallback, TrainingArguments, TrainerState
from genrec.trainers.generative.tiger_trainer import GenerativeTrainer
from genrec.utils.nni_utils import get_nni_params, update_config_with_nni, report_nni_metrics
from genrec.generation.trie import Trie,prefix_allowed_tokens_fn
import matplotlib.pyplot as plt
import seaborn as sns

def plot_encoder_attention_heatmap(
    attention_weights,
    input_ids,
    file_path="encoder_self_attention_heatmap.png",
    head_index=None, # æ–°å¢å‚æ•°ï¼Œç”¨äºåœ¨æ ‡é¢˜ä¸­æ³¨æ˜æ˜¯å“ªä¸ªå¤´
    highlight_diagonal=True,
    draw_grid_lines=True
):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜å•ä¸ªEncoderè‡ªæ³¨æ„åŠ›çƒ­åŠ›å›¾ã€‚
    """
    # ... (è¿™éƒ¨åˆ†ä»£ç ä¸ä¹‹å‰å®Œå…¨ç›¸åŒï¼Œç”¨äºå‡†å¤‡æ•°æ®)
    if isinstance(attention_weights, torch.Tensor):
        attention_weights = attention_weights.cpu().numpy()
    if isinstance(input_ids, torch.Tensor):
        input_ids = input_ids.cpu().tolist()
    non_pad_tokens_indices = [i for i, token_id in enumerate(input_ids) if token_id != 0]
    if not non_pad_tokens_indices:
        print(f"è­¦å‘Šï¼šæ ·æœ¬åªåŒ…å«padding tokensï¼Œè·³è¿‡ç»˜å›¾ã€‚æ–‡ä»¶è·¯å¾„: {file_path}")
        return
    attention_weights = attention_weights[non_pad_tokens_indices, :][:, non_pad_tokens_indices]
    labels = [str(input_ids[i]) for i in non_pad_tokens_indices]
    seq_len = len(labels)

    fig, ax = plt.subplots(figsize=(10, 8))

    sns.heatmap(
        attention_weights, xticklabels=labels, yticklabels=labels,
        cmap="viridis", annot=False, ax=ax
    )

    # --- ä¿®æ”¹ç‚¹ï¼šåŠ¨æ€ç”Ÿæˆæ ‡é¢˜ ---
    title = "Encoder Self-Attention Heatmap"
    if head_index is not None:
        title += f" (Head {head_index})"
    else:
        title += " (Averaged Heads)"
    ax.set_title(title)
    # --- ç»“æŸä¿®æ”¹ç‚¹ ---

    if highlight_diagonal:
        ax.plot([0, seq_len], [0, seq_len], color='red', linestyle='--', linewidth=1.5)

    if draw_grid_lines and seq_len > 1:
        line_positions = [1] + [i + 1 for i in range(1, seq_len) if (i - 1) % 4 == 3]
        for pos in line_positions:
            if pos < seq_len:
                ax.axvline(x=pos, color='white', linestyle=':', linewidth=0.8)
                ax.axhline(y=pos, color='white', linestyle=':', linewidth=0.8)

    ax.set_xlabel("Key (attended to)")
    ax.set_ylabel("Query (attending from)")
    ax.tick_params(axis='x', rotation=45)
    ax.tick_params(axis='y', rotation=0)
    fig.tight_layout()

    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    plt.savefig(file_path)
    plt.close(fig)
import numpy as np
import os
def calculate_item_attention_v2(attention_matrix, seq_len):
    """
    è®¡ç®—å¹¶æ¯”è¾ƒ item å†…éƒ¨æ³¨æ„åŠ›å’Œ item é—´æ³¨æ„åŠ›çš„å¹³å‡å€¼ (ç‰ˆæœ¬2)ã€‚

    è§„åˆ™:
    - å¿½ç•¥åºåˆ—çš„ç¬¬ä¸€ä¸ª token (index 0)ã€‚
    - ä»ç¬¬äºŒä¸ª token å¼€å§‹ï¼Œæ¯ 4 ä¸ª token æ„æˆä¸€ä¸ª itemã€‚
    - Intra-item (å†…éƒ¨) æ³¨æ„åŠ›: 
        - ç‰ˆæœ¬1 (åŒ…å«è‡ªèº«): ä¸€ä¸ª item å†…çš„ token åˆ°è¯¥ item å†…æ‰€æœ‰ token (åŒ…æ‹¬è‡ªèº«) çš„æ³¨æ„åŠ›ã€‚
        - ç‰ˆæœ¬2 (ä¸å«è‡ªèº«): ä¸€ä¸ª item å†…çš„ token åˆ°è¯¥ item å†…å…¶ä»– token çš„æ³¨æ„åŠ›ã€‚
    - Inter-item (ä¹‹é—´) æ³¨æ„åŠ›: ä¸€ä¸ª item å†…çš„ token åˆ°æ‰€æœ‰å…¶ä»– items å†…æ‰€æœ‰ token çš„æ³¨æ„åŠ› (åŒå‘)ã€‚

    å‚æ•°:
    - attention_matrix (np.array): (seq_len, seq_len) çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚
    - seq_len (int): åºåˆ—çš„æœ‰æ•ˆé•¿åº¦ (ä¸åŒ…å« padding)ã€‚

    è¿”å›:
    - dict: åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸ã€‚
    """
    intra_item_scores_with_self = []
    intra_item_scores_without_self = []
    inter_item_scores = []

    # ä» token 1 å¼€å§‹è®¡ç®—ï¼Œæ‰€ä»¥æœ‰æ•ˆ item token æ•°é‡ä¸º seq_len - 1
    num_items = (seq_len - 1) // 4

    # å¦‚æœ item ä¸è¶³ä¸¤ä¸ªï¼Œåˆ™æ— æ³•è®¡ç®— item é—´æ³¨æ„åŠ›
    if num_items < 2:
        # ä»ç„¶å¯ä»¥è®¡ç®—å•ä¸ª item çš„å†…éƒ¨æ³¨æ„åŠ›
        if num_items == 1:
            start_idx = 1
            end_idx = min(1 + 4, seq_len)
            item_indices = list(range(start_idx, end_idx))
            if len(item_indices) > 1:
                for q_idx in item_indices:
                    for k_idx in item_indices:
                        score = attention_matrix[q_idx, k_idx]
                        intra_item_scores_with_self.append(score)
                        if q_idx != k_idx:
                            intra_item_scores_without_self.append(score)
        
        avg_intra_with_self = np.mean(intra_item_scores_with_self) if intra_item_scores_with_self else 0
        avg_intra_without_self = np.mean(intra_item_scores_without_self) if intra_item_scores_without_self else 0
        
        return {
            "avg_intra_item_score_with_self": avg_intra_with_self,
            "avg_intra_item_score_without_self": avg_intra_without_self,
            "avg_inter_item_score": 0,
            "comparison_with_self": "N/A (ä¸è¶³2ä¸ªitem)",
            "comparison_without_self": "N/A (ä¸è¶³2ä¸ªitem)",
            "intra_item_scores_with_self_count": len(intra_item_scores_with_self),
            "intra_item_scores_without_self_count": len(intra_item_scores_without_self),
            "inter_item_scores_count": 0
        }

    # é¢„å…ˆè®¡ç®—æ‰€æœ‰ item çš„ç´¢å¼•èŒƒå›´
    item_indices_list = []
    for i in range(num_items):
        start_idx = 1 + 4 * i
        end_idx = min(start_idx + 4, seq_len)
        item_indices_list.append(list(range(start_idx, end_idx)))

    # éå†æ¯ä¸ª item ä½œä¸º "æº" item (ä»ä¸­é€‰å– query)
    for k in range(num_items):
        query_item_indices = item_indices_list[k]
        
        # éå†æ‰€æœ‰ item ä½œä¸º "ç›®æ ‡" item (ä»ä¸­é€‰å– key)
        for j in range(num_items):
            key_item_indices = item_indices_list[j]

            # --- æ ¹æ®æºå’Œç›®æ ‡æ˜¯å¦ç›¸åŒï¼Œåˆ†é…åˆ°ä¸åŒåˆ—è¡¨ ---
            if k == j:  # Intra-item (å†…éƒ¨) æ³¨æ„åŠ›
                if len(query_item_indices) < 2: continue
                for q_idx in query_item_indices:
                    for k_idx in key_item_indices:
                        score = attention_matrix[q_idx, k_idx]
                        # ç‰ˆæœ¬1: åŒ…å«å¯¹è‡ªèº«çš„æ³¨æ„åŠ›
                        intra_item_scores_with_self.append(score)
                        # ç‰ˆæœ¬2: ä¸åŒ…å«å¯¹è‡ªèº«çš„æ³¨æ„åŠ›
                        if q_idx != k_idx:
                            intra_item_scores_without_self.append(score)
            else:  # Inter-item (ä¹‹é—´) æ³¨æ„åŠ›
                for q_idx in query_item_indices:
                    for k_idx in key_item_indices:
                        inter_item_scores.append(attention_matrix[q_idx, k_idx])

    # --- è®¡ç®—å¹³å‡å€¼ ---
    avg_intra_with_self = np.mean(intra_item_scores_with_self) if intra_item_scores_with_self else 0
    avg_intra_without_self = np.mean(intra_item_scores_without_self) if intra_item_scores_without_self else 0
    avg_inter = np.mean(inter_item_scores) if inter_item_scores else 0

    # --- è¿›è¡Œæ¯”è¾ƒ ---
    def get_comparison(score1, score2, name1, name2):
        if score1 > score2: return f"{name1} > {name2}"
        if score2 > score1: return f"{name2} > {name1}"
        return f"{name1} == {name2}"

    comp_with_self = get_comparison(avg_intra_with_self, avg_inter, "å¹³å‡å†…éƒ¨æ³¨æ„åŠ›(å«è‡ªèº«)", "å¹³å‡è·¨itemæ³¨æ„åŠ›")
    comp_without_self = get_comparison(avg_intra_without_self, avg_inter, "å¹³å‡å†…éƒ¨æ³¨æ„åŠ›(ä¸å«è‡ªèº«)", "å¹³å‡è·¨itemæ³¨æ„åŠ›")

    return {
        "avg_intra_item_score_with_self": avg_intra_with_self,
        "avg_intra_item_score_without_self": avg_intra_without_self,
        "avg_inter_item_score": avg_inter,
        "comparison_with_self": comp_with_self,
        "comparison_without_self": comp_without_self,
        "intra_item_scores_with_self_count": len(intra_item_scores_with_self),
        "intra_item_scores_without_self_count": len(intra_item_scores_without_self),
        "inter_item_scores_count": len(inter_item_scores)
    }
def calculate_token_level_attention_v3(attention_matrix, seq_len):
    """
    ä»¥"Tokençº§å¹³å‡"çš„è§†è§’ï¼Œè®¡ç®—ä¸‰ç§æ³¨æ„åŠ›æŒ‡æ ‡ã€‚

    1.  Avg Intra-Item: å¯¹æ¯ä¸ªtokenï¼Œè®¡ç®—å®ƒå¯¹è‡ªèº«itemå†…å…¶ä»–tokençš„å¹³å‡æ³¨æ„åŠ›ï¼Œ
        ç„¶åå°†è¿™äº›å¹³å‡å€¼åœ¨æ‰€æœ‰tokenä¸Šå†æ¬¡å¹³å‡ã€‚
    2.  Avg Inter-Item: å¯¹æ¯ä¸ªtokenï¼Œè®¡ç®—å®ƒå¯¹æ‰€æœ‰å¤–éƒ¨itemså†…æ‰€æœ‰tokençš„å¹³å‡æ³¨æ„åŠ›ï¼Œ
        ç„¶åå°†è¿™äº›å¹³å‡å€¼åœ¨æ‰€æœ‰tokenä¸Šå†æ¬¡å¹³å‡ã€‚
    3.  Avg Attention To Each Item: å¯¹äºæ¯ä¸ªç›®æ ‡item Jï¼Œè®¡ç®—æ‰€æœ‰tokenå¯¹Jå†…æ‰€æœ‰tokençš„
        å¹³å‡æ³¨æ„åŠ›ã€‚è¿™ä¼šè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªç›®æ ‡itemã€‚

    å‚æ•°:
    - attention_matrix (np.array): (seq_len, seq_len) çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚
    - seq_len (int): åºåˆ—çš„æœ‰æ•ˆé•¿åº¦ (ä¸åŒ…å« padding)ã€‚

    è¿”å›:
    - dict: åŒ…å«ä¸‰ç§åˆ†æç»“æœçš„å­—å…¸ã€‚
    """
    if seq_len <= 1:
        return {}

    num_items = (seq_len - 1) // 4
    if num_items == 0:
        return {}

    # --- æ­¥éª¤1: å‡†å¤‡è¾…åŠ©æ•°æ®ç»“æ„ ---
    item_indices_list = []
    token_to_item_map = {}  # åˆ›å»ºä¸€ä¸ªä»tokenç´¢å¼•åˆ°å…¶itemç´¢å¼•çš„æ˜ å°„
    for i in range(num_items):
        start_idx = 1 + 4 * i
        end_idx = min(start_idx + 4, seq_len)
        indices = list(range(start_idx, end_idx))
        item_indices_list.append(indices)
        for token_idx in indices:
            token_to_item_map[token_idx] = i

    all_item_tokens_indices = list(range(1, seq_len))
    if not all_item_tokens_indices:
        return {}
        
    # --- æ­¥éª¤2: è®¡ç®— Tokençº§ Intra-å’Œ Inter- æ³¨æ„åŠ› ---
    per_token_avg_intra_scores = []
    per_token_avg_inter_scores = []

    # ä»æ¯ä¸ªquery_tokençš„è§†è§’å‡ºå‘
    for q_idx in all_item_tokens_indices:
        q_item_idx = token_to_item_map[q_idx]
        
        # æ‰¾å‡ºå½“å‰tokençš„å†…éƒ¨å’Œå¤–éƒ¨key token
        intra_k_indices = [k for k in item_indices_list[q_item_idx] if k != q_idx]
        inter_k_indices = [k for k in all_item_tokens_indices if token_to_item_map[k] != q_item_idx]

        # è®¡ç®—å¯¹å†…çš„å¹³å‡æ³¨æ„åŠ›
        if intra_k_indices:
            avg_intra = np.mean([attention_matrix[q_idx, k_idx] for k_idx in intra_k_indices])
            per_token_avg_intra_scores.append(avg_intra)
        
        # è®¡ç®—å¯¹å¤–çš„å¹³å‡æ³¨æ„åŠ›
        if inter_k_indices:
            avg_inter = np.mean([attention_matrix[q_idx, k_idx] for k_idx in inter_k_indices])
            per_token_avg_inter_scores.append(avg_inter)

    # åœ¨æ‰€æœ‰tokenä¸Šæ±‚æœ€ç»ˆå¹³å‡
    final_avg_intra = np.mean(per_token_avg_intra_scores) if per_token_avg_intra_scores else 0
    final_avg_inter = np.mean(per_token_avg_inter_scores) if per_token_avg_inter_scores else 0
    
    # --- æ­¥éª¤3: è®¡ç®—å¯¹æ¯ä¸ªç‰¹å®šItemçš„å¹³å‡æ³¨æ„åŠ› ---
    attention_to_each_item = []
    for j in range(num_items): # éå†æ¯ä¸ªitemä½œä¸º"ç›®æ ‡"
        target_item_indices = item_indices_list[j]
        
        # è®¡ç®—æ‰€æœ‰æºtokenåˆ°è¿™ä¸ªç›®æ ‡itemçš„å¹³å‡æ³¨æ„åŠ›
        all_scores_to_item_j = []
        for q_idx in all_item_tokens_indices:
            # ä¸åŒºåˆ†q_idxæ˜¯å¦åœ¨ç›®æ ‡itemå†…ï¼Œè®¡ç®—å®ƒå¯¹ç›®æ ‡itemçš„æ³¨æ„åŠ›
            scores = [attention_matrix[q_idx, k_idx] for k_idx in target_item_indices]
            if scores:
                 all_scores_to_item_j.extend(scores)

        avg_attention_to_j = np.mean(all_scores_to_item_j) if all_scores_to_item_j else 0
        attention_to_each_item.append({
            "target_item_index": j,
            "avg_attention_score": avg_attention_to_j
        })

    return {
        "token_level_avg_intra_item_attention (without_self)": final_avg_intra,
        "token_level_avg_inter_item_attention": final_avg_inter,
        "avg_attention_to_each_item": attention_to_each_item,
        "comparison": f"Intra ({final_avg_intra:.4f}) {' >' if final_avg_intra > final_avg_inter else ' <'} Inter ({final_avg_inter:.4f})"
    }
import json
def extract_and_save_vectors(model, output_file_path="vocab_vectors.txt"):
    """
    (æ›´å¥å£®çš„ç‰ˆæœ¬) ä»æ¨¡å‹ä¸­æå–è¯æ±‡è¡¨çš„æ‰€æœ‰å‘é‡ï¼Œå¹¶ä»¥æ–‡æœ¬æ ¼å¼ä¿å­˜ã€‚
    æ­¤ç‰ˆæœ¬ä¸ä¾èµ– tokenizerï¼Œç›´æ¥é€šè¿‡ embedding å±‚çš„ shape è¿›è¡Œéå†ï¼Œ
    é€‚ç”¨äº token å°±æ˜¯å…¶æ•´æ•° ID çš„æƒ…å†µã€‚

    Args:
        model: ä½ è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è±¡ã€‚
        output_file_path (str): ä¿å­˜å‘é‡æ–‡ä»¶çš„è·¯å¾„ã€‚
    """
    print("ğŸš€ å¼€å§‹æå–è¯æ±‡è¡¨å‘é‡ (å¥å£®æ¨¡å¼)...")
    
    # 1. è·å–æ¨¡å‹çš„è¯åµŒå…¥å±‚
    try:
        word_embeddings = model.get_input_embeddings()
        if word_embeddings is None:
            print("âŒ é”™è¯¯ï¼šæ— æ³•ä»æ¨¡å‹ä¸­è·å–è¯åµŒå…¥å±‚ (model.get_input_embeddings() is None)ã€‚")
            return
    except Exception as e:
        print(f"âŒ è·å–è¯åµŒå…¥å±‚æ—¶å‡ºé”™: {e}")
        return

    # 2. è·å–åµŒå…¥çŸ©é˜µï¼Œå¹¶è½¬ç§»åˆ°CPU
    embedding_matrix = word_embeddings.weight.detach().cpu().numpy()
    
    # 3. ç›´æ¥ä»çŸ©é˜µå½¢çŠ¶è·å–è¯æ±‡é‡å’Œç»´åº¦
    num_embeddings, embedding_dim = embedding_matrix.shape
    print(f"æ£€æµ‹åˆ°è¯æ±‡é‡: {num_embeddings}, å‘é‡ç»´åº¦: {embedding_dim}")

    # 4. å°†å‘é‡å†™å…¥æ–‡ä»¶
    try:
        with open(output_file_path, 'w', encoding='utf-8') as f:
            # å†™å…¥æ–‡ä»¶å¤´
            f.write(f"{num_embeddings} {embedding_dim}\n")
            
            # ç›´æ¥æŒ‰ç´¢å¼• (token_id) éå†
            for token_id in range(num_embeddings):
                vector = embedding_matrix[token_id]
                vector_str = ' '.join(map(str, vector))
                # å°† token_id æœ¬èº«ä½œä¸º "token" å†™å…¥
                f.write(f"{token_id} {vector_str}\n")
        
        print(f"âœ… æˆåŠŸå°† {num_embeddings} ä¸ªå‘é‡å†™å…¥åˆ°: '{os.path.abspath(output_file_path)}'")

    except IOError as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶æ—¶å‘ç”ŸIOé”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ å†™å…¥è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
from itertools import combinations
def _get_historical_items(input_ids_tensor, tokenizer):
    """
    è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†è¾“å…¥çš„ token ID åºåˆ—è§£ææˆå†å² item åˆ—è¡¨ã€‚
    æ–°è§„åˆ™:
    1. ç§»é™¤æ‰€æœ‰ padding tokens.
    2. åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆ token æ˜¯ user tokenï¼Œå°†è¢«å¿½ç•¥.
    3. ä¹‹åæ¯ 4 ä¸ª token æ„æˆä¸€ä¸ª item.
    """
    pad_token_id = 0

    # 1. è½¬æ¢ä¸ºåˆ—è¡¨å¹¶ç§»é™¤ padding
    tokens = [t for t in input_ids_tensor.tolist() if t != pad_token_id]

    # 2. æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ token (è‡³å°‘1ä¸ªuser token + 4ä¸ªitem token)ï¼Œå¹¶ç§»é™¤ user token
    if len(tokens) < 5: # è‡³å°‘éœ€è¦ user token + 1ä¸ªitem
        return []

    # å¿½ç•¥ç¬¬ä¸€ä¸ª token (user token)
    item_tokens = tokens[1:]

    # 3. å°†å‰©ä½™çš„ token æ¯ 4 ä¸ªä¸€ç»„è¿›è¡Œåˆ‡åˆ†
    history = []
    num_item_tokens = len(item_tokens)
    for i in range(0, num_item_tokens, 4):
        # ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªå®Œæ•´çš„ 4-token item
        if i + 4 <= num_item_tokens:
            history.append(item_tokens[i:i + 4])
            
    return history

import csv
def calculate_and_log_attention_stats(
    attention_matrix, 
    input_ids, 
    tokenizer, 
    output_csv_path, 
    item_size=4,
    pad_token_id=0
):
    """
    è®¡ç®—å¹¶è®°å½•æ³¨æ„åŠ›ç»Ÿè®¡ï¼ˆæœ€ç»ˆä¿®æ­£ç‰ˆï¼Œå·²é€‚é…å·¦paddingï¼Œå¸¦è°ƒè¯•ä¿¡æ¯ï¼‰ã€‚
    """
    print(f"   [è°ƒè¯•] æ¥æ”¶åˆ° (å·¦padding) input_ids (å‰30ä¸ª): {input_ids[:30]}")

    # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šé€‚é…å·¦padding ---
    # ä»å¤´å¼€å§‹éå†ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªâ€œéâ€padding tokençš„ç´¢å¼•ï¼Œè¿™æ‰æ˜¯æœ‰æ•ˆåºåˆ—çš„å¼€å§‹
    valid_start_index = -1
    for i, token_id in enumerate(input_ids):
        if token_id != pad_token_id:
            valid_start_index = i
            break
    
    # --- è°ƒè¯•ï¼šæ‰“å°è®¡ç®—å‡ºçš„åºåˆ—ä¿¡æ¯ ---
    print(f"   [è°ƒè¯•] ç¬¬ä¸€ä¸ªæœ‰æ•ˆtokençš„ç´¢å¼• (valid_start_index): {valid_start_index}")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆtokenï¼ˆæ•´ä¸ªåºåˆ—éƒ½æ˜¯paddingï¼‰ï¼Œæˆ–åªæœ‰ä¸€ä¸ªtoken
    if valid_start_index == -1 or (len(input_ids) - valid_start_index) <= 1:
        real_seq_len = 0 if valid_start_index == -1 else (len(input_ids) - valid_start_index)
        print(f"   [è°ƒè¯•] çœŸå®åºåˆ—é•¿åº¦ <= 1 (é•¿åº¦ä¸º {real_seq_len})ï¼Œæ— æ³•è¿›è¡Œitemåˆ†æï¼Œå‡½æ•°æå‰è¿”å›(0,0,0)ã€‚")
        with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['error'])
            writer.writerow([f'No valid tokens to analyze. Real sequence length was {real_seq_len}.'])
        return 0, 0, 0

    # æ ¹æ®æœ‰æ•ˆåºåˆ—çš„èµ·ç‚¹ï¼Œå¯¹æ•°æ®è¿›è¡Œåˆ‡ç‰‡
    valid_input_ids = input_ids[valid_start_index:]
    real_seq_len = len(valid_input_ids)
    
    if hasattr(attention_matrix, 'cpu'):
        attention_matrix_np = attention_matrix.cpu().numpy()
    else:
        attention_matrix_np = attention_matrix
        
    # æ³¨æ„åŠ›çŸ©é˜µä¹Ÿè¦è¿›è¡ŒåŒæ ·çš„åˆ‡ç‰‡
    valid_attention_matrix = attention_matrix_np[valid_start_index:, valid_start_index:]
    
    print(f"   [è°ƒè¯•] åˆ‡ç‰‡åçš„çœŸå®åºåˆ—é•¿åº¦: {real_seq_len}")
    print(f"   [è°ƒè¯•] åˆ‡ç‰‡åçš„ attention_matrix å½¢çŠ¶: {valid_attention_matrix.shape}")

    token_stats = []
    
    # åç§»é‡ä»ç„¶æ˜¯1ï¼Œå› ä¸ºæˆ‘ä»¬è¦è·³è¿‡æœ‰æ•ˆåºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªtokenï¼ˆuser tokenï¼‰
    offset = 1

    # åœ¨â€œæœ‰æ•ˆåºåˆ—â€çš„èŒƒå›´å†…è¿›è¡Œéå†
    for query_idx in range(offset, real_seq_len):
        query_token_id = valid_input_ids[query_idx]
        query_token = query_token_id

        item_index = (query_idx - offset) // item_size
        item_start_idx = offset + item_index * item_size
        item_end_idx = min(item_start_idx + item_size, real_seq_len)

        own_item_scores_with_self = []
        own_item_scores_no_self = []
        other_item_scores = []

        for key_idx in range(offset, real_seq_len):
            score = valid_attention_matrix[query_idx, key_idx]

            if item_start_idx <= key_idx < item_end_idx:
                own_item_scores_with_self.append(score)
                if query_idx != key_idx:
                    own_item_scores_no_self.append(score)
            else:
                other_item_scores.append(score)
        
        avg_own_with_self = np.mean(own_item_scores_with_self) if own_item_scores_with_self else 0
        avg_own_no_self = np.mean(own_item_scores_no_self) if own_item_scores_no_self else 0
        avg_other = np.mean(other_item_scores) if other_item_scores else 0
        
        token_stats.append({
            # æ³¨æ„ï¼šè¿™é‡Œçš„ç´¢å¼•æ˜¯ç›¸å¯¹äºæœ‰æ•ˆåºåˆ—çš„ï¼Œè€Œä¸æ˜¯åŸå§‹åºåˆ—
            'token_index_in_valid_seq': query_idx,
            'token': query_token,
            'item_index': item_index,
            'avg_attention_to_own_item (with self)': f"{avg_own_with_self:.6f}",
            'avg_attention_to_own_item (no self)': f"{avg_own_no_self:.6f}",
            'avg_attention_to_other_items': f"{avg_other:.6f}"
        })

    if not token_stats:
        print("   [è°ƒè¯•] å¾ªç¯ç»“æŸä½†æœªæ”¶é›†åˆ°ä»»ä½•tokenç»Ÿè®¡æ•°æ®ï¼Œè¿”å›(0,0,0)ã€‚")
        return 0, 0, 0
            
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=token_stats[0].keys())
        writer.writeheader()
        writer.writerows(token_stats)
        
    total_avg_own_with_self = np.mean([float(s['avg_attention_to_own_item (with self)']) for s in token_stats])
    total_avg_own_no_self = np.mean([float(s['avg_attention_to_own_item (no self)']) for s in token_stats])
    total_avg_other = np.mean([float(s['avg_attention_to_other_items']) for s in token_stats])

    return total_avg_own_with_self, total_avg_own_no_self, total_avg_other

vis_config = {
    "shared_prefix_1": 5,
    "shared_prefix_2": 5,
    "shared_prefix_3": 5,
    "shared_token_no_prefix": 5,
    "other_samples": 5
}