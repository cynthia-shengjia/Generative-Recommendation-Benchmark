from typing import Optional, Dict, List, Callable
from functools import partial  
from transformers import TrainingArguments, EarlyStoppingCallback  
from genrec.utils.metrics import compute_metrics  
from genrec.trainers.online_rl.grpo_trainer import GRPOTrainer  
from genrec.utils.callbacks.generative.generative_callback import (  
    GenerativeLoggingCallback,  
    EvaluateEveryNEpochsCallback  
)  
from genrec.utils.models_setup.conditional_t5_setup import create_t5_model  

import math


def create_grpo_reward_function(use_ndcg=True, ndcg_weight=0.5):   
    """   
    åˆ›å»º GRPO çš„å¥–åŠ±å‡½æ•°  
      
    Args:   
        tokenizer: TigerTokenizer å®ä¾‹  
        use_ndcg: æ˜¯å¦ä½¿ç”¨ NDCG å¥–åŠ±
        ndcg_weight: NDCG å¥–åŠ±çš„æƒé‡ (0-1ä¹‹é—´)
      
    Returns:   
        reward_func: å¥–åŠ±å‡½æ•°  
    """   
    def reward_func(generated_items: List[int], target_items: List[int], 
                   num_generations: int) -> List[float]:   
        """   
        å¥–åŠ±å‡½æ•°ï¼šç»“åˆåŒ¹é…å¥–åŠ±å’Œ NDCG å¥–åŠ±
        æ³¨æ„ï¼šgenerated_items å·²ç»æŒ‰ç…§ beam search çš„åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
          
        Args:   
            generated_items: ç”Ÿæˆçš„ç‰©å“ ID åˆ—è¡¨ [B * num_generations]
            target_items: ç›®æ ‡ç‰©å“ ID åˆ—è¡¨ [B * num_generations]
            num_generations: æ¯ä¸ªæ ·æœ¬çš„ç”Ÿæˆæ•°é‡
          
        Returns:   
            rewards: å¥–åŠ±åˆ—è¡¨  
        """   
        # é¢„è®¡ç®— NDCG è´Ÿå¥–åŠ±ï¼ˆåªéœ€è®¡ç®—ä¸€æ¬¡ï¼‰
        ndcg_penalties = [-1.0/math.log2(i+2) for i in range(num_generations)]
        ndcg_sum = sum(ndcg_penalties)
        ndcg_penalties = [-elm/ndcg_sum for elm in ndcg_penalties]
        
        rewards = []
        
        # æŒ‰ç»„å¤„ç†ï¼ˆæ¯ç»„æœ‰ num_generations ä¸ªç”Ÿæˆç»“æœï¼‰
        for group_idx in range(len(generated_items) // num_generations):
            start_idx = group_idx * num_generations
            end_idx = start_idx + num_generations
            
            # è·å–å½“å‰ç»„çš„æ•°æ®
            group_gen_items = generated_items[start_idx:end_idx]
            group_target_items = target_items[start_idx:end_idx]
            
            # æ³¨æ„ï¼šgroup_gen_items å·²ç»æŒ‰ç…§æ¦‚ç‡ä»é«˜åˆ°ä½æ’åº
            # rank 0 æ˜¯æ¦‚ç‡æœ€é«˜çš„ï¼Œrank num_generations-1 æ˜¯æ¦‚ç‡æœ€ä½çš„
            for rank, (gen_item, target_item) in enumerate(zip(group_gen_items, group_target_items)):
                # åŸºç¡€åŒ¹é…å¥–åŠ±
                match_reward = 1.0 if gen_item == target_item else 0.0
                
                if not use_ndcg:
                    # ä¸ä½¿ç”¨ NDCGï¼Œåªç”¨åŒ¹é…å¥–åŠ±
                    final_reward = match_reward
                else:
                    if match_reward == 1.0:  # æ­£æ ·æœ¬
                        # æ­£æ ·æœ¬çš„ NDCG å¥–åŠ±ä¸º 0
                        final_reward = (1 - ndcg_weight) * match_reward + ndcg_weight * 0.0
                    else:  # è´Ÿæ ·æœ¬
                        # è´Ÿæ ·æœ¬æ ¹æ®æ’åè·å¾—è´Ÿå¥–åŠ±
                        # rank è¶Šå¤§ï¼ˆæ’åè¶Šé åï¼‰ï¼Œæƒ©ç½šè¶Šå°ï¼ˆç»å¯¹å€¼ï¼‰
                        final_reward = (1 - ndcg_weight) * match_reward + ndcg_weight * ndcg_penalties[rank]
                
                rewards.append(final_reward)
        return rewards

      
    return reward_func

def create_trainer(  
    model,  
    training_args,  
    train_dataset,  
    eval_dataset,  
    data_collator,
    # é€šç”¨å‚æ•°  
    callbacks: Optional[List] = None,  
    # S-DPO ç‰¹æœ‰å‚æ•°  
    ref_model: Optional = None,  
    beta: float = 0.1,  
    num_generations: int = 2,
    # ç”Ÿæˆè¯„ä¼°å‚æ•°  
    compute_metrics: Optional[callable] = None,  
    generation_params: Optional[Dict] = None,  
    item2tokens: Optional[Dict] = None,  
    tokens2item: Optional[Dict] = None,  
    pad_token_id: Optional[int] = None,  
    eos_token_id: Optional[int] = None,  
    reward_func: Optional[Callable] = None,
    **kwargs  
):  
    """  
    åˆ›å»º GRPOTrainer çš„å·¥å‚å‡½æ•°  
      
    Args:  
        model: ç­–ç•¥æ¨¡å‹
        training_args: è®­ç»ƒå‚æ•°
        train_dataset: è®­ç»ƒæ•°æ®é›†ï¼ˆåŒ…å« chosen/rejected labelsï¼‰
        eval_dataset: è¯„ä¼°æ•°æ®é›†ï¼ˆåªéœ€è¦ chosen_labelsï¼‰
        data_collator: è®­ç»ƒæ•°æ® collator
        callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨
        ref_model: å‚è€ƒæ¨¡å‹ï¼ˆç”¨äº S-DPOï¼‰
        beta: S-DPO æ¸©åº¦å‚æ•°
        compute_metrics: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•°
        generation_params: ç”Ÿæˆå‚æ•°ï¼ˆmax_gen_length, num_beams, max_kï¼‰
        item2tokens: item åˆ° token çš„æ˜ å°„ï¼ˆç”¨äºå‰ç¼€çº¦æŸï¼‰
        pad_token_id: pad token id
        eos_token_id: eos token id
    """  
      
    # æ£€æŸ¥å¿…éœ€å‚æ•°  
    if ref_model is None:  
        raise ValueError("ä½¿ç”¨ GRPOTrainer æ—¶éœ€è¦æä¾› ref_model å‚æ•°")  
    
    if None in [compute_metrics, generation_params, item2tokens, pad_token_id, eos_token_id]:
        raise ValueError("ä½¿ç”¨ GRPOTrainer è¿›è¡Œç”Ÿæˆè¯„ä¼°æ—¶éœ€è¦æä¾› compute_metrics, "
                       "generation_params, item2tokens, pad_token_id å’Œ eos_token_id å‚æ•°")
      
    return GRPOTrainer(  
        model=model,  
        ref_model = ref_model,
        beta=beta,  
        num_generations = num_generations,
        args=training_args,  
        train_dataset=train_dataset,  
        eval_dataset=eval_dataset,  
        data_collator=data_collator,  
        callbacks=callbacks,  
        # ğŸ”´ ç”Ÿæˆè¯„ä¼°å‚æ•°  
        compute_metrics=compute_metrics,  
        generation_params=generation_params,  
        item2tokens=item2tokens,  
        tokens2item=tokens2item,
        pad_token_id=pad_token_id,  
        eos_token_id=eos_token_id,  
        reward_func = reward_func,
        **kwargs  
    )  

def setup_training(  
    model,   
    tokenizer,   
    train_dataset,   
    valid_dataset,   
    model_config,   
    online_rl_config,  
    output_dirs,   
    logger,   
    per_device_train_batch_size,  
    per_device_eval_batch_size,   
    train_data_collator,  
):  
    """
    è®¾ç½® GRPO è®­ç»ƒ
    
    Args:
        model: ç­–ç•¥æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        train_dataset: è®­ç»ƒæ•°æ®é›†
        valid_dataset: éªŒè¯æ•°æ®é›†
        model_config: æ¨¡å‹é…ç½®
        online_rl_config: ç¦»çº¿å¼ºåŒ–å­¦ä¹ é…ç½®
        output_dirs: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
        per_device_train_batch_size: è®­ç»ƒæ‰¹æ¬¡å¤§å°
        per_device_eval_batch_size: è¯„ä¼°æ‰¹æ¬¡å¤§å°
        train_data_collator: è®­ç»ƒæ•°æ® collatorï¼ˆå¤„ç† chosen/rejectedï¼‰
    """
    
    # ===== 1. è®­ç»ƒå‚æ•°é…ç½® =====
    training_args = TrainingArguments(  
        output_dir=output_dirs['model'],  
        num_train_epochs=model_config['num_epochs'],  
        per_device_train_batch_size=per_device_train_batch_size,  
        per_device_eval_batch_size=per_device_eval_batch_size,  
        learning_rate=model_config['learning_rate'],  
        weight_decay=model_config["weight_decay"],  
        eval_strategy="epoch",  
        save_strategy="epoch",  
        save_total_limit=2,  
        load_best_model_at_end=True,  
        logging_dir=output_dirs['logs'],  
        logging_steps=100,  
        report_to=[],  
        warmup_ratio=model_config["warmup_ratio"],  
        ddp_find_unused_parameters=False,  
        remove_unused_columns=False,  
        # ğŸ”´ è¯„ä¼°æŒ‡æ ‡é…ç½®ï¼ˆä½¿ç”¨ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ï¼‰
        metric_for_best_model="ndcg@10",  # æˆ– "recall@10"
        greater_is_better=True,  
    )  
    
    # ===== 2. ç”Ÿæˆè¯„ä¼°å‚æ•° =====
    tokens_to_item_map = tokenizer.tokens2item  
    compute_metrics_with_map = partial(
        compute_metrics, 
        tokens_to_item_map=tokens_to_item_map
    )  
      
    num_beams = model_config.get('num_beams', 10)  
    max_gen_length = model_config.get('max_gen_length', 5)  
    k_list = model_config.get('k_list', [5, 10, 20])  
    max_k = k_list[-1] if k_list else 10
      
    generation_params = {  
        'max_gen_length': max_gen_length,  
        'num_beams': num_beams,  
        'max_k': max_k  
    }  
    
    # ===== 3. å›è°ƒå‡½æ•° =====
    callbacks = [  
        EarlyStoppingCallback(
            early_stopping_patience=model_config.get("early_stop_upper_steps", 1000)
        ),   
        GenerativeLoggingCallback(logger),   
        EvaluateEveryNEpochsCallback(
            n_epochs=model_config.get("evaluation_epoch", 5)
        )  
    ]  
    
    # ===== 4. åˆ›å»ºå‚è€ƒæ¨¡å‹ =====
    logger.info("åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆReference Modelï¼‰...")
    ref_model = create_t5_model(  
        vocab_size=tokenizer.vocab_size,  
        model_config=model_config  
    )  
    # ğŸ”´ åŠ è½½ä¸ç­–ç•¥æ¨¡å‹ç›¸åŒçš„æƒé‡  
    ref_model.load_state_dict(model.state_dict())  
    ref_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    # ğŸ”´ å†»ç»“å‚è€ƒæ¨¡å‹å‚æ•°
    for param in ref_model.parameters():
        param.requires_grad = False
    logger.info("å‚è€ƒæ¨¡å‹åˆ›å»ºå®Œæˆ")
    


    reward_func = create_grpo_reward_function(use_ndcg=True, ndcg_weight=0.5)

    # ===== 5. åˆ›å»º Trainer =====
    trainer = create_trainer(  
        model=model,  
        training_args=training_args,  
        train_dataset=train_dataset,  
        eval_dataset=valid_dataset,  
        data_collator=train_data_collator,  
        callbacks=callbacks,  
        # S-DPO å‚æ•°  
        ref_model=ref_model,  
        beta=online_rl_config.get('beta', 0.1),  
        num_generations = online_rl_config.get("num_generations",2),
        # ç”Ÿæˆè¯„ä¼°å‚æ•°  
        compute_metrics=compute_metrics_with_map,  
        generation_params=generation_params,  
        item2tokens=tokenizer.item2tokens,  
        tokens2item=tokenizer.tokens2item,
        pad_token_id=tokenizer.pad_token,  
        eos_token_id=tokenizer.eos_token,  
        reward_func=reward_func
    )  
    
    logger.info(f"Trainer é…ç½®å®Œæˆ:")
    logger.info(f"  - Beta: {online_rl_config.get('beta', 0.1)}")
    logger.info(f"  - Num beams: {num_beams}")
    logger.info(f"  - Max gen length: {max_gen_length}")
    logger.info(f"  - Max k: {max_k}")
    logger.info(f"  - Metric for best model: {training_args.metric_for_best_model}")
      
    return trainer