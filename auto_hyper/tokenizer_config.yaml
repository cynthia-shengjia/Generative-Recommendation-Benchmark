# nnictl create --config ./auto_hyper/tokenizer_config.yaml --port 8070
experimentName: Instruments
max_trial_number: 100

searchSpaceFile: tokenizer_search_space.json

# trialCommand: python onlyRQVAE.py dataset="Instruments" output_dir="./output"  tokenizer.learning_rate=0.1 tokenizer.epochs=20000  tokenizer.final_model_decision="save_final"

# train LETTER command
trialCommand: python onlyLETTER.py dataset="Beauty" output_dir="./output"  tokenizer.cf_emb_path="/home/lz/code/Generative-Recommendation-Benchmark/cf_embs/beauty_cf_emb.pt" tokenizer.cf_alpha=0.02 tokenizer.diversity_beta=0.0001 tokenizer.learning_rate=0.1 tokenizer.epochs=20000  tokenizer.final_model_decision="save_final"

trialCodeDirectory: ..

trialGpuNumber: 1       # 每块 GPU 上多少个实验
trial_concurrency: 4    # 多少个实验一起跑


tuner:
  name: TPE
  classArgs:
    optimize_mode: maximize

trainingService:
  platform: local
  maxTrialNumberPerGpu: 1
  useActiveGpu: true
  gpuIndices: [0, 1, 5, 6]