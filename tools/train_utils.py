import os
import torch
import torch.optim as optim
from tqdm import tqdm
from datetime import datetime
from accelerate import Accelerator
from transformers import get_linear_schedule_with_warmup
from genrec.models.MyCustomT5.CustomT5 import CustomT5ForConditionalGeneration
from genrec.tokenizers.TigerTokenizer import TigerTokenizer
from tools.utils import calc_ndcg, tokens_to_item_id
from typing import Optional, Dict, Any, List, Union, Callable  # æ·»åŠ äº†å¿…è¦çš„å¯¼å…¥
import math
import logging
import re
from transformers import T5ForConditionalGeneration,T5Config,Trainer
import numpy as np
import math
from transformers import EvalPrediction
from functools import partial
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback
from transformers import TrainerCallback, TrainingArguments, TrainerState
from trainers.model_trainers.GenerativeTrainer import GenerativeTrainer
from tools.nni_utils import get_nni_params, update_config_with_nni, report_nni_metrics
from genrec.cbs_structure.generate_trie import Trie,prefix_allowed_tokens_fn
import matplotlib.pyplot as plt
import seaborn as sns
def create_t5_model(vocab_size: int, model_config: dict) -> T5ForConditionalGeneration:
    """
    åˆ›å»ºæ ‡å‡†çš„T5æ¨¡å‹ï¼Œæ ¹æ®æä¾›çš„é…ç½®å‚æ•°
    """
    config = T5Config(
        vocab_size=vocab_size,
        d_model = model_config['d_model'],  # è®¡ç®— d_model
        d_kv=model_config['d_kv'],
        d_ff=model_config['d_ff'],
        num_layers=model_config['num_layers'],
        num_decoder_layers=model_config['num_decoder_layers'],
        num_heads=model_config['num_heads'],
        dropout_rate=model_config['dropout_rate'],
        tie_word_embeddings=model_config['tie_word_embeddings'],
        pad_token_id=0,  # æ ¹æ®æ‚¨çš„tokenizerè®¾ç½®
        eos_token_id=1,   # æ ¹æ®æ‚¨çš„tokenizerè®¾ç½®
        decoder_start_token_id=0,  # é€šå¸¸ä¸pad_token_idç›¸åŒ
    )
    
    model = T5ForConditionalGeneration(config)
    return model

def create_custom_t5_model(vocab_size: int, model_config: dict, tokens_per_item:int,num_items:int,max_seq_len:int) -> CustomT5ForConditionalGeneration:
    config = T5Config(
        vocab_size=vocab_size,
        d_model = model_config['d_model'],  # è®¡ç®— d_model
        d_kv=model_config['d_kv'],
        d_ff=model_config['d_ff'],
        num_layers=model_config['num_layers'],
        num_decoder_layers=model_config['num_decoder_layers'],
        num_heads=model_config['num_heads'],
        dropout_rate=model_config['dropout_rate'],
        tie_word_embeddings=model_config['tie_word_embeddings'],
        pad_token_id=0,
        eos_token_id=1,
        decoder_start_token_id=0,
        tokens_per_item=tokens_per_item,
        num_items=num_items,
        max_seq_len = max_seq_len
    )
    
    model = CustomT5ForConditionalGeneration(config)
    return model


def plot_encoder_attention_heatmap(
    attention_weights,
    input_ids,
    file_path="encoder_self_attention_heatmap.png",
    head_index=None, # æ–°å¢å‚æ•°ï¼Œç”¨äºåœ¨æ ‡é¢˜ä¸­æ³¨æ˜æ˜¯å“ªä¸ªå¤´
    highlight_diagonal=True,
    draw_grid_lines=True
):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜å•ä¸ªEncoderè‡ªæ³¨æ„åŠ›çƒ­åŠ›å›¾ã€‚
    """
    # ... (è¿™éƒ¨åˆ†ä»£ç ä¸ä¹‹å‰å®Œå…¨ç›¸åŒï¼Œç”¨äºå‡†å¤‡æ•°æ®)
    if isinstance(attention_weights, torch.Tensor):
        attention_weights = attention_weights.cpu().numpy()
    if isinstance(input_ids, torch.Tensor):
        input_ids = input_ids.cpu().tolist()
    non_pad_tokens_indices = [i for i, token_id in enumerate(input_ids) if token_id != 0]
    if not non_pad_tokens_indices:
        print(f"è­¦å‘Šï¼šæ ·æœ¬åªåŒ…å«padding tokensï¼Œè·³è¿‡ç»˜å›¾ã€‚æ–‡ä»¶è·¯å¾„: {file_path}")
        return
    attention_weights = attention_weights[non_pad_tokens_indices, :][:, non_pad_tokens_indices]
    labels = [str(input_ids[i]) for i in non_pad_tokens_indices]
    seq_len = len(labels)

    fig, ax = plt.subplots(figsize=(10, 8))

    sns.heatmap(
        attention_weights, xticklabels=labels, yticklabels=labels,
        cmap="viridis", annot=False, ax=ax
    )

    # --- ä¿®æ”¹ç‚¹ï¼šåŠ¨æ€ç”Ÿæˆæ ‡é¢˜ ---
    title = "Encoder Self-Attention Heatmap"
    if head_index is not None:
        title += f" (Head {head_index})"
    else:
        title += " (Averaged Heads)"
    ax.set_title(title)
    # --- ç»“æŸä¿®æ”¹ç‚¹ ---

    if highlight_diagonal:
        ax.plot([0, seq_len], [0, seq_len], color='red', linestyle='--', linewidth=1.5)

    if draw_grid_lines and seq_len > 1:
        line_positions = [1] + [i + 1 for i in range(1, seq_len) if (i - 1) % 4 == 3]
        for pos in line_positions:
            if pos < seq_len:
                ax.axvline(x=pos, color='white', linestyle=':', linewidth=0.8)
                ax.axhline(y=pos, color='white', linestyle=':', linewidth=0.8)

    ax.set_xlabel("Key (attended to)")
    ax.set_ylabel("Query (attending from)")
    ax.tick_params(axis='x', rotation=45)
    ax.tick_params(axis='y', rotation=0)
    fig.tight_layout()

    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    plt.savefig(file_path)
    plt.close(fig)
import numpy as np
import os
def calculate_item_attention_v2(attention_matrix, seq_len):
    """
    è®¡ç®—å¹¶æ¯”è¾ƒ item å†…éƒ¨æ³¨æ„åŠ›å’Œ item é—´æ³¨æ„åŠ›çš„å¹³å‡å€¼ (ç‰ˆæœ¬2)ã€‚

    è§„åˆ™:
    - å¿½ç•¥åºåˆ—çš„ç¬¬ä¸€ä¸ª token (index 0)ã€‚
    - ä»ç¬¬äºŒä¸ª token å¼€å§‹ï¼Œæ¯ 4 ä¸ª token æ„æˆä¸€ä¸ª itemã€‚
    - Intra-item (å†…éƒ¨) æ³¨æ„åŠ›: 
        - ç‰ˆæœ¬1 (åŒ…å«è‡ªèº«): ä¸€ä¸ª item å†…çš„ token åˆ°è¯¥ item å†…æ‰€æœ‰ token (åŒ…æ‹¬è‡ªèº«) çš„æ³¨æ„åŠ›ã€‚
        - ç‰ˆæœ¬2 (ä¸å«è‡ªèº«): ä¸€ä¸ª item å†…çš„ token åˆ°è¯¥ item å†…å…¶ä»– token çš„æ³¨æ„åŠ›ã€‚
    - Inter-item (ä¹‹é—´) æ³¨æ„åŠ›: ä¸€ä¸ª item å†…çš„ token åˆ°æ‰€æœ‰å…¶ä»– items å†…æ‰€æœ‰ token çš„æ³¨æ„åŠ› (åŒå‘)ã€‚

    å‚æ•°:
    - attention_matrix (np.array): (seq_len, seq_len) çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚
    - seq_len (int): åºåˆ—çš„æœ‰æ•ˆé•¿åº¦ (ä¸åŒ…å« padding)ã€‚

    è¿”å›:
    - dict: åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸ã€‚
    """
    intra_item_scores_with_self = []
    intra_item_scores_without_self = []
    inter_item_scores = []

    # ä» token 1 å¼€å§‹è®¡ç®—ï¼Œæ‰€ä»¥æœ‰æ•ˆ item token æ•°é‡ä¸º seq_len - 1
    num_items = (seq_len - 1) // 4

    # å¦‚æœ item ä¸è¶³ä¸¤ä¸ªï¼Œåˆ™æ— æ³•è®¡ç®— item é—´æ³¨æ„åŠ›
    if num_items < 2:
        # ä»ç„¶å¯ä»¥è®¡ç®—å•ä¸ª item çš„å†…éƒ¨æ³¨æ„åŠ›
        if num_items == 1:
            start_idx = 1
            end_idx = min(1 + 4, seq_len)
            item_indices = list(range(start_idx, end_idx))
            if len(item_indices) > 1:
                for q_idx in item_indices:
                    for k_idx in item_indices:
                        score = attention_matrix[q_idx, k_idx]
                        intra_item_scores_with_self.append(score)
                        if q_idx != k_idx:
                            intra_item_scores_without_self.append(score)
        
        avg_intra_with_self = np.mean(intra_item_scores_with_self) if intra_item_scores_with_self else 0
        avg_intra_without_self = np.mean(intra_item_scores_without_self) if intra_item_scores_without_self else 0
        
        return {
            "avg_intra_item_score_with_self": avg_intra_with_self,
            "avg_intra_item_score_without_self": avg_intra_without_self,
            "avg_inter_item_score": 0,
            "comparison_with_self": "N/A (ä¸è¶³2ä¸ªitem)",
            "comparison_without_self": "N/A (ä¸è¶³2ä¸ªitem)",
            "intra_item_scores_with_self_count": len(intra_item_scores_with_self),
            "intra_item_scores_without_self_count": len(intra_item_scores_without_self),
            "inter_item_scores_count": 0
        }

    # é¢„å…ˆè®¡ç®—æ‰€æœ‰ item çš„ç´¢å¼•èŒƒå›´
    item_indices_list = []
    for i in range(num_items):
        start_idx = 1 + 4 * i
        end_idx = min(start_idx + 4, seq_len)
        item_indices_list.append(list(range(start_idx, end_idx)))

    # éå†æ¯ä¸ª item ä½œä¸º "æº" item (ä»ä¸­é€‰å– query)
    for k in range(num_items):
        query_item_indices = item_indices_list[k]
        
        # éå†æ‰€æœ‰ item ä½œä¸º "ç›®æ ‡" item (ä»ä¸­é€‰å– key)
        for j in range(num_items):
            key_item_indices = item_indices_list[j]

            # --- æ ¹æ®æºå’Œç›®æ ‡æ˜¯å¦ç›¸åŒï¼Œåˆ†é…åˆ°ä¸åŒåˆ—è¡¨ ---
            if k == j:  # Intra-item (å†…éƒ¨) æ³¨æ„åŠ›
                if len(query_item_indices) < 2: continue
                for q_idx in query_item_indices:
                    for k_idx in key_item_indices:
                        score = attention_matrix[q_idx, k_idx]
                        # ç‰ˆæœ¬1: åŒ…å«å¯¹è‡ªèº«çš„æ³¨æ„åŠ›
                        intra_item_scores_with_self.append(score)
                        # ç‰ˆæœ¬2: ä¸åŒ…å«å¯¹è‡ªèº«çš„æ³¨æ„åŠ›
                        if q_idx != k_idx:
                            intra_item_scores_without_self.append(score)
            else:  # Inter-item (ä¹‹é—´) æ³¨æ„åŠ›
                for q_idx in query_item_indices:
                    for k_idx in key_item_indices:
                        inter_item_scores.append(attention_matrix[q_idx, k_idx])

    # --- è®¡ç®—å¹³å‡å€¼ ---
    avg_intra_with_self = np.mean(intra_item_scores_with_self) if intra_item_scores_with_self else 0
    avg_intra_without_self = np.mean(intra_item_scores_without_self) if intra_item_scores_without_self else 0
    avg_inter = np.mean(inter_item_scores) if inter_item_scores else 0

    # --- è¿›è¡Œæ¯”è¾ƒ ---
    def get_comparison(score1, score2, name1, name2):
        if score1 > score2: return f"{name1} > {name2}"
        if score2 > score1: return f"{name2} > {name1}"
        return f"{name1} == {name2}"

    comp_with_self = get_comparison(avg_intra_with_self, avg_inter, "å¹³å‡å†…éƒ¨æ³¨æ„åŠ›(å«è‡ªèº«)", "å¹³å‡è·¨itemæ³¨æ„åŠ›")
    comp_without_self = get_comparison(avg_intra_without_self, avg_inter, "å¹³å‡å†…éƒ¨æ³¨æ„åŠ›(ä¸å«è‡ªèº«)", "å¹³å‡è·¨itemæ³¨æ„åŠ›")

    return {
        "avg_intra_item_score_with_self": avg_intra_with_self,
        "avg_intra_item_score_without_self": avg_intra_without_self,
        "avg_inter_item_score": avg_inter,
        "comparison_with_self": comp_with_self,
        "comparison_without_self": comp_without_self,
        "intra_item_scores_with_self_count": len(intra_item_scores_with_self),
        "intra_item_scores_without_self_count": len(intra_item_scores_without_self),
        "inter_item_scores_count": len(inter_item_scores)
    }
def calculate_token_level_attention_v3(attention_matrix, seq_len):
    """
    ä»¥"Tokençº§å¹³å‡"çš„è§†è§’ï¼Œè®¡ç®—ä¸‰ç§æ³¨æ„åŠ›æŒ‡æ ‡ã€‚

    1.  Avg Intra-Item: å¯¹æ¯ä¸ªtokenï¼Œè®¡ç®—å®ƒå¯¹è‡ªèº«itemå†…å…¶ä»–tokençš„å¹³å‡æ³¨æ„åŠ›ï¼Œ
        ç„¶åå°†è¿™äº›å¹³å‡å€¼åœ¨æ‰€æœ‰tokenä¸Šå†æ¬¡å¹³å‡ã€‚
    2.  Avg Inter-Item: å¯¹æ¯ä¸ªtokenï¼Œè®¡ç®—å®ƒå¯¹æ‰€æœ‰å¤–éƒ¨itemså†…æ‰€æœ‰tokençš„å¹³å‡æ³¨æ„åŠ›ï¼Œ
        ç„¶åå°†è¿™äº›å¹³å‡å€¼åœ¨æ‰€æœ‰tokenä¸Šå†æ¬¡å¹³å‡ã€‚
    3.  Avg Attention To Each Item: å¯¹äºæ¯ä¸ªç›®æ ‡item Jï¼Œè®¡ç®—æ‰€æœ‰tokenå¯¹Jå†…æ‰€æœ‰tokençš„
        å¹³å‡æ³¨æ„åŠ›ã€‚è¿™ä¼šè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªç›®æ ‡itemã€‚

    å‚æ•°:
    - attention_matrix (np.array): (seq_len, seq_len) çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚
    - seq_len (int): åºåˆ—çš„æœ‰æ•ˆé•¿åº¦ (ä¸åŒ…å« padding)ã€‚

    è¿”å›:
    - dict: åŒ…å«ä¸‰ç§åˆ†æç»“æœçš„å­—å…¸ã€‚
    """
    if seq_len <= 1:
        return {}

    num_items = (seq_len - 1) // 4
    if num_items == 0:
        return {}

    # --- æ­¥éª¤1: å‡†å¤‡è¾…åŠ©æ•°æ®ç»“æ„ ---
    item_indices_list = []
    token_to_item_map = {}  # åˆ›å»ºä¸€ä¸ªä»tokenç´¢å¼•åˆ°å…¶itemç´¢å¼•çš„æ˜ å°„
    for i in range(num_items):
        start_idx = 1 + 4 * i
        end_idx = min(start_idx + 4, seq_len)
        indices = list(range(start_idx, end_idx))
        item_indices_list.append(indices)
        for token_idx in indices:
            token_to_item_map[token_idx] = i

    all_item_tokens_indices = list(range(1, seq_len))
    if not all_item_tokens_indices:
        return {}
        
    # --- æ­¥éª¤2: è®¡ç®— Tokençº§ Intra-å’Œ Inter- æ³¨æ„åŠ› ---
    per_token_avg_intra_scores = []
    per_token_avg_inter_scores = []

    # ä»æ¯ä¸ªquery_tokençš„è§†è§’å‡ºå‘
    for q_idx in all_item_tokens_indices:
        q_item_idx = token_to_item_map[q_idx]
        
        # æ‰¾å‡ºå½“å‰tokençš„å†…éƒ¨å’Œå¤–éƒ¨key token
        intra_k_indices = [k for k in item_indices_list[q_item_idx] if k != q_idx]
        inter_k_indices = [k for k in all_item_tokens_indices if token_to_item_map[k] != q_item_idx]

        # è®¡ç®—å¯¹å†…çš„å¹³å‡æ³¨æ„åŠ›
        if intra_k_indices:
            avg_intra = np.mean([attention_matrix[q_idx, k_idx] for k_idx in intra_k_indices])
            per_token_avg_intra_scores.append(avg_intra)
        
        # è®¡ç®—å¯¹å¤–çš„å¹³å‡æ³¨æ„åŠ›
        if inter_k_indices:
            avg_inter = np.mean([attention_matrix[q_idx, k_idx] for k_idx in inter_k_indices])
            per_token_avg_inter_scores.append(avg_inter)

    # åœ¨æ‰€æœ‰tokenä¸Šæ±‚æœ€ç»ˆå¹³å‡
    final_avg_intra = np.mean(per_token_avg_intra_scores) if per_token_avg_intra_scores else 0
    final_avg_inter = np.mean(per_token_avg_inter_scores) if per_token_avg_inter_scores else 0
    
    # --- æ­¥éª¤3: è®¡ç®—å¯¹æ¯ä¸ªç‰¹å®šItemçš„å¹³å‡æ³¨æ„åŠ› ---
    attention_to_each_item = []
    for j in range(num_items): # éå†æ¯ä¸ªitemä½œä¸º"ç›®æ ‡"
        target_item_indices = item_indices_list[j]
        
        # è®¡ç®—æ‰€æœ‰æºtokenåˆ°è¿™ä¸ªç›®æ ‡itemçš„å¹³å‡æ³¨æ„åŠ›
        all_scores_to_item_j = []
        for q_idx in all_item_tokens_indices:
            # ä¸åŒºåˆ†q_idxæ˜¯å¦åœ¨ç›®æ ‡itemå†…ï¼Œè®¡ç®—å®ƒå¯¹ç›®æ ‡itemçš„æ³¨æ„åŠ›
            scores = [attention_matrix[q_idx, k_idx] for k_idx in target_item_indices]
            if scores:
                 all_scores_to_item_j.extend(scores)

        avg_attention_to_j = np.mean(all_scores_to_item_j) if all_scores_to_item_j else 0
        attention_to_each_item.append({
            "target_item_index": j,
            "avg_attention_score": avg_attention_to_j
        })

    return {
        "token_level_avg_intra_item_attention (without_self)": final_avg_intra,
        "token_level_avg_inter_item_attention": final_avg_inter,
        "avg_attention_to_each_item": attention_to_each_item,
        "comparison": f"Intra ({final_avg_intra:.4f}) {' >' if final_avg_intra > final_avg_inter else ' <'} Inter ({final_avg_inter:.4f})"
    }
import json
def extract_and_save_vectors(model, output_file_path="vocab_vectors.txt"):
    """
    (æ›´å¥å£®çš„ç‰ˆæœ¬) ä»æ¨¡å‹ä¸­æå–è¯æ±‡è¡¨çš„æ‰€æœ‰å‘é‡ï¼Œå¹¶ä»¥æ–‡æœ¬æ ¼å¼ä¿å­˜ã€‚
    æ­¤ç‰ˆæœ¬ä¸ä¾èµ– tokenizerï¼Œç›´æ¥é€šè¿‡ embedding å±‚çš„ shape è¿›è¡Œéå†ï¼Œ
    é€‚ç”¨äº token å°±æ˜¯å…¶æ•´æ•° ID çš„æƒ…å†µã€‚

    Args:
        model: ä½ è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è±¡ã€‚
        output_file_path (str): ä¿å­˜å‘é‡æ–‡ä»¶çš„è·¯å¾„ã€‚
    """
    print("ğŸš€ å¼€å§‹æå–è¯æ±‡è¡¨å‘é‡ (å¥å£®æ¨¡å¼)...")
    
    # 1. è·å–æ¨¡å‹çš„è¯åµŒå…¥å±‚
    try:
        word_embeddings = model.get_input_embeddings()
        if word_embeddings is None:
            print("âŒ é”™è¯¯ï¼šæ— æ³•ä»æ¨¡å‹ä¸­è·å–è¯åµŒå…¥å±‚ (model.get_input_embeddings() is None)ã€‚")
            return
    except Exception as e:
        print(f"âŒ è·å–è¯åµŒå…¥å±‚æ—¶å‡ºé”™: {e}")
        return

    # 2. è·å–åµŒå…¥çŸ©é˜µï¼Œå¹¶è½¬ç§»åˆ°CPU
    embedding_matrix = word_embeddings.weight.detach().cpu().numpy()
    
    # 3. ç›´æ¥ä»çŸ©é˜µå½¢çŠ¶è·å–è¯æ±‡é‡å’Œç»´åº¦
    num_embeddings, embedding_dim = embedding_matrix.shape
    print(f"æ£€æµ‹åˆ°è¯æ±‡é‡: {num_embeddings}, å‘é‡ç»´åº¦: {embedding_dim}")

    # 4. å°†å‘é‡å†™å…¥æ–‡ä»¶
    try:
        with open(output_file_path, 'w', encoding='utf-8') as f:
            # å†™å…¥æ–‡ä»¶å¤´
            f.write(f"{num_embeddings} {embedding_dim}\n")
            
            # ç›´æ¥æŒ‰ç´¢å¼• (token_id) éå†
            for token_id in range(num_embeddings):
                vector = embedding_matrix[token_id]
                vector_str = ' '.join(map(str, vector))
                # å°† token_id æœ¬èº«ä½œä¸º "token" å†™å…¥
                f.write(f"{token_id} {vector_str}\n")
        
        print(f"âœ… æˆåŠŸå°† {num_embeddings} ä¸ªå‘é‡å†™å…¥åˆ°: '{os.path.abspath(output_file_path)}'")

    except IOError as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶æ—¶å‘ç”ŸIOé”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ å†™å…¥è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
from itertools import combinations
def _get_historical_items(input_ids_tensor, tokenizer):
    """
    è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†è¾“å…¥çš„ token ID åºåˆ—è§£ææˆå†å² item åˆ—è¡¨ã€‚
    æ–°è§„åˆ™:
    1. ç§»é™¤æ‰€æœ‰ padding tokens.
    2. åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆ token æ˜¯ user tokenï¼Œå°†è¢«å¿½ç•¥.
    3. ä¹‹åæ¯ 4 ä¸ª token æ„æˆä¸€ä¸ª item.
    """
    pad_token_id = 0

    # 1. è½¬æ¢ä¸ºåˆ—è¡¨å¹¶ç§»é™¤ padding
    tokens = [t for t in input_ids_tensor.tolist() if t != pad_token_id]

    # 2. æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ token (è‡³å°‘1ä¸ªuser token + 4ä¸ªitem token)ï¼Œå¹¶ç§»é™¤ user token
    if len(tokens) < 5: # è‡³å°‘éœ€è¦ user token + 1ä¸ªitem
        return []

    # å¿½ç•¥ç¬¬ä¸€ä¸ª token (user token)
    item_tokens = tokens[1:]

    # 3. å°†å‰©ä½™çš„ token æ¯ 4 ä¸ªä¸€ç»„è¿›è¡Œåˆ‡åˆ†
    history = []
    num_item_tokens = len(item_tokens)
    for i in range(0, num_item_tokens, 4):
        # ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªå®Œæ•´çš„ 4-token item
        if i + 4 <= num_item_tokens:
            history.append(item_tokens[i:i + 4])
            
    return history

# def evaluate_model_with_constrained_beam_search(
#     model,
#     eval_dataloader,
#     accelerator,
#     tokenizer,
#     k_list=[1, 5, 10],
#     num_beams=10,
#     max_gen_length=5,
#     logger=None,
#     mode="Evaluation",
#     num_visualize_samples=10,
#     visualization_dir="attention_heatmaps",
#     output_json_path="predictions.json",
#     vector_output_path="vocab_vectors.txt",
# ):
#     """
#     è¯„ä¼°æ¨¡å‹æˆ–å¯è§†åŒ–æ³¨æ„åŠ›ã€‚
#     - num_visualize_samples > 0: æ¿€æ´»å¯è§†åŒ–æ¨¡å¼ã€‚
#     - visualize_individual_heads = True: ä¸ºæ¯ä¸ªå¤´å•ç‹¬ç”»å›¾ã€‚
#     - visualize_individual_heads = False: åªç”»æ‰€æœ‰å¤´çš„å¹³å‡å›¾ã€‚
#     """
#     # if accelerator.is_main_process:
#     #     extract_and_save_vectors(model, output_file_path=vector_output_path)
    
#     # accelerator.wait_for_everyone()
    
#     # print("å‘é‡æå–ä»»åŠ¡å®Œæˆï¼Œå‡½æ•°å°†æå‰é€€å‡ºï¼Œä¸æ‰§è¡Œåç»­è¯„ä¼°ã€‚")
#     # return
#     model.eval()

#     if num_visualize_samples > 0:
#         all_samples_metrics = []
#         if accelerator.is_main_process:
#             print(f"è¿›å…¥ä»…å¯è§†åŒ–æ¨¡å¼ï¼Œå°†ä¸º {num_visualize_samples} ä¸ªæ ·æœ¬ç»˜åˆ¶çƒ­åŠ›å›¾...")
#             os.makedirs(visualization_dir, exist_ok=True)
#             print(f"çƒ­åŠ›å›¾å°†ä¿å­˜åˆ°: '{visualization_dir}/'")

#         device = accelerator.device
#         samples_drawn = 0
#         with torch.no_grad():
#             for batch_idx, batch in enumerate(eval_dataloader):
#                 if samples_drawn >= num_visualize_samples: break
#                 # ... (generate a output is same as before)
#                 input_ids = batch['input_ids'].to(device)
#                 attention_mask = batch['attention_mask'].to(device)
#                 outputs = model.generate(
#                     input_ids=input_ids, attention_mask=attention_mask, max_length=2,
#                     num_beams=1, return_dict_in_generate=True, output_attentions=True
#                 )
#                 if not (hasattr(outputs, 'encoder_attentions') and outputs.encoder_attentions is not None):
#                     if accelerator.is_main_process:
#                         print("é”™è¯¯: æ¨¡å‹è¾“å‡ºä¸­æœªæ‰¾åˆ° 'encoder_attentions'ã€‚")
#                     return
                
#                 last_layer_attentions = outputs.encoder_attentions[-1] # Shape: (batch, heads, seq, seq)
                
#                 for i in range(input_ids.size(0)):
#                     if samples_drawn >= num_visualize_samples: break

#                     if accelerator.is_main_process:
#                         print(f"\n--- æ­£åœ¨åˆ†æ Batch {batch_idx}, Sample {i} ---")
#                         sample_attentions = last_layer_attentions[i] # Shape: (heads, seq, seq)
#                         sample_input_ids = input_ids[i].cpu().tolist()
                        
#                         # è·å–æœ‰æ•ˆåºåˆ—é•¿åº¦ (ä¸å«padding)
#                         non_pad_indices = [idx for idx, token_id in enumerate(sample_input_ids) if token_id != 0]
#                         effective_seq_len = len(non_pad_indices)

#                         # --- 1. è®¡ç®—ä¸åˆ†æ (åŸºäºå¹³å‡æ³¨æ„åŠ›) ---
#                         avg_attention_weights = sample_attentions.mean(dim=0)
#                         effective_attention_matrix = avg_attention_weights[-effective_seq_len:, -effective_seq_len:]
#                         # è°ƒç”¨æ–°å‡½æ•°è®¡ç®—æŒ‡æ ‡
#                         metrics = calculate_token_level_attention_v3(
#                             effective_attention_matrix.cpu().numpy(), 
#                             effective_seq_len,
#                             #  f"batch_{batch_idx}_sample_{i}"
#                         )
#                         metrics["sample_id"] = f"batch_{batch_idx}_sample_{i}"
#                         all_samples_metrics.append(metrics)

#                         # æ‰“å°è®¡ç®—ç»“æœ
#                         if not metrics:
#                             print("  - åºåˆ—æœ‰æ•ˆtokenä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚")
#                         else:
#                             # æ‰“å°Intra vs Interçš„æ¯”è¾ƒ
#                             print(f"  - Tokençº§å¹³å‡å†…éƒ¨æ³¨æ„åŠ›: {metrics['token_level_avg_intra_item_attention (without_self)']:.4f}")
#                             print(f"  - Tokençº§å¹³å‡å¤–éƒ¨æ³¨æ„åŠ›: {metrics['token_level_avg_inter_item_attention']:.4f}")
#                             print(f"  - æ¯”è¾ƒç»“æœ: {metrics['comparison']}")
                            
#                             # æ‰“å°å¯¹æ¯ä¸ªitemçš„å¹³å‡æ³¨æ„åŠ›
#                             print("  - å¯¹æ¯ä¸ªç›®æ ‡Itemçš„å¹³å‡æ³¨æ„åŠ›:")
#                             for item_data in metrics['avg_attention_to_each_item']:
#                                 print(f"    - å¯¹Item {item_data['target_item_index']}: {item_data['avg_attention_score']:.4f}")
                        
#                         # print(f"åˆ†æç»“æœ:")
#                         # print(f"  - å¹³å‡è¿‘è·ç¦»åˆ†æ•°: {metrics['avg_near_score']:.4f} (åŸºäº {metrics['near_scores_count']} ä¸ªåˆ†æ•°)")
#                         # print(f"  - å¹³å‡è¿œè·ç¦»åˆ†æ•°: {metrics['avg_far_score']:.4f} (åŸºäº {metrics['far_scores_count']} ä¸ªåˆ†æ•°)")
#                         # print(f"  - æ¯”è¾ƒç»“è®º: {metrics['comparison']}")
#                         sample_output_dir = os.path.join(visualization_dir, f"batch_{batch_idx}_sample_{i}")
#                         os.makedirs(sample_output_dir, exist_ok=True)
#                         avg_file_path = os.path.join(sample_output_dir, "heatmap_average.png")
#                         plot_encoder_attention_heatmap(avg_attention_weights, input_ids[i], file_path=avg_file_path)

#                         num_heads = sample_attentions.shape[0]
#                         for head_idx in range(num_heads):
#                             head_attention_weights = sample_attentions[head_idx] # Shape: (seq, seq)
#                             file_path = os.path.join(sample_output_dir, f"head_{head_idx}_heatmap.png")
                            
#                             plot_encoder_attention_heatmap(
#                                 head_attention_weights,
#                                 sample_input_ids,
#                                 file_path=file_path,
#                                 head_index=head_idx # ä¼ å…¥å½“å‰å¤´çš„ç´¢å¼•
#                             )
                    
#                     samples_drawn += 1
                
#         if accelerator.is_main_process:
#             print(f"å·²æˆåŠŸä¸º {samples_drawn} ä¸ªæ ·æœ¬ç”Ÿæˆå¯è§†åŒ–å›¾åƒã€‚")
#         return
#     tokens_to_item_map = tokenizer.tokens2item
#     item_to_tokens_map = tokenizer.item2tokens
#     candidate_trie = Trie(tokenizer.item2tokens)
#     prefix_allowed_fn = prefix_allowed_tokens_fn(candidate_trie)
#     all_predictions = []
#     all_labels = []
#     device = accelerator.device
#     progress_bar = tqdm(eval_dataloader, desc=f"{mode}", disable=not accelerator.is_main_process)
#     for batch_idx, batch in enumerate(progress_bar):
#         input_ids = batch['input_ids'].to(device)
#         attention_mask = batch['attention_mask'].to(device)
#         true_item_ids = batch['label_id']

#         outputs = model.generate(
#             input_ids=input_ids,
#             attention_mask=attention_mask,
#             max_length=max_gen_length,
#             num_beams=num_beams,
#             num_return_sequences=num_beams,
#             early_stopping=True,
#             pad_token_id=0,
#             eos_token_id=1,
#             output_scores=True,
#             return_dict_in_generate=True,
#             prefix_allowed_tokens_fn=prefix_allowed_fn
#         )
#         generated_ids = outputs.sequences
#         sequences_scores = outputs.sequences_scores
#         batch_size = input_ids.size(0)
#         generated_ids_reshaped = generated_ids.view(batch_size, num_beams, -1)[:, :, 1:]
#         probabilities_reshaped = torch.exp(sequences_scores).view(batch_size, num_beams)
#         batch_predictions = []
#         batch_labels = []
#         for i in range(batch_size):
#             true_item_id = true_item_ids[i]
#             true_item_tokens = item_to_tokens_map.get(true_item_id, []) 
#             # batch_labels.append(true_item_id)
#             batch_labels.append({
#                 'id': true_item_id,
#                 'tokens': true_item_tokens
#             })
#             user_sequences = generated_ids_reshaped[i]
#             user_probs = probabilities_reshaped[i]
#             sorted_indices = torch.argsort(user_probs, descending=True)
#             sorted_sequences = user_sequences[sorted_indices]
#             # item_ids = []
#             # for seq in sorted_sequences:
#             #     item_id = tokens_to_item_id(seq.tolist(), tokens_to_item_map)
#             #     item_ids.append(item_id)
#             predictions_with_tokens = []
#             for seq in sorted_sequences:
#                 pred_tokens = seq.tolist()
#                 item_id = tokens_to_item_id(pred_tokens, tokens_to_item_map)
#                 predictions_with_tokens.append({
#                     'id': item_id,
#                     'tokens': pred_tokens
#                 })
#             unique_predictions = []
#             seen_ids = set()
#             for pred in predictions_with_tokens:
#                 if pred['id'] not in seen_ids:
#                     unique_predictions.append(pred)
#                     seen_ids.add(pred['id'])
#             # item_ids = [x for x in item_ids if not (x in seen or seen.add(x))]
#             # batch_predictions.append(item_ids)
#             batch_predictions.append(unique_predictions)
#         gathered_predictions = accelerator.gather_for_metrics(batch_predictions)
#         gathered_labels = accelerator.gather_for_metrics(batch_labels)
#         all_predictions.extend(gathered_predictions)
#         all_labels.extend(gathered_labels)

#     if accelerator.is_main_process:
#         metrics = {f'hit@{k}': 0 for k in k_list}
#         metrics.update({f'ndcg@{k}': 0 for k in k_list})
#         total_samples = len(all_labels)
#         results_to_save = []
#         for label_info, prediction_list in zip(all_labels, all_predictions):
            
#             true_item_id = label_info['id']
#             # ä»é¢„æµ‹ç»“æœï¼ˆå­—å…¸åˆ—è¡¨ï¼‰ä¸­æå–å‡º ID åˆ—è¡¨ï¼Œç”¨äºè®¡ç®—æŒ‡æ ‡
#             predicted_item_ids = [p['id'] for p in prediction_list]

#             # --- æŒ‡æ ‡è®¡ç®—ä»£ç  ---
#             for k in k_list:
#                 top_k_ids = predicted_item_ids[:k]
#                 hit = 1 if true_item_id in top_k_ids else 0
#                 metrics[f'hit@{k}'] += hit
#                 if hit:
#                     rank = top_k_ids.index(true_item_id) + 1
#                     ndcg = 1 / math.log2(rank + 1)
#                     metrics[f'ndcg@{k}'] += ndcg
            
#             # ===== æ”¹åŠ¨ 5: æ„å»ºæ›´ä¸°å¯Œçš„ JSON å¯¹è±¡ =====
#             sample_result = {
#                 'label_id': label_info['id'],
#                 'label_tokens': label_info['tokens'],
#                 'predictions': [
#                     {
#                         'predicted_id': p['id'],
#                         'predicted_tokens': p['tokens']
#                     } for p in prediction_list # ä¿å­˜æ‰€æœ‰å»é‡åçš„é¢„æµ‹ç»“æœ
#                 ]
#             }
#             results_to_save.append(sample_result)
#         # for true_item_id, predicted_items in zip(all_labels, all_predictions):
#         #     for k in k_list:
#         #         top_k_items = predicted_items[:k]
#         #         hit = 1 if true_item_id in top_k_items else 0
#         #         metrics[f'hit@{k}'] += hit
#         #         if hit:
#         #             rank = top_k_items.index(true_item_id) + 1
#         #             ndcg = 1 / math.log2(rank + 1)
#         #             metrics[f'ndcg@{k}'] += ndcg
#         #     sample_result = {
#         #         'label': true_item_id,
#         #         'top10_predictions': predicted_items
#         #     }
#         #     results_to_save.append(sample_result) # å°†å­—å…¸æ·»åŠ åˆ°åˆ—è¡¨ä¸­
#         final_metrics = {}
#         for key, value in metrics.items():
#             final_metrics[key] = value / total_samples

#         if logger:
#             logger.info(f"\n{mode} ç»“æœ (å…± {total_samples} ä¸ªæ ·æœ¬):")
#             for k in k_list:
#                 logger.info(f"Hit@{k}: {final_metrics[f'hit@{k}']:.4f}, NDCG@{k}: {final_metrics[f'ndcg@{k}']:.4f}")
#         with open(output_json_path, 'w', encoding='utf-8') as f:
#             json.dump(results_to_save, f, indent=4, ensure_ascii=False)
        
#         if logger:
#             logger.info(f"Top-10 é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_json_path}")
#         return final_metrics.
import csv
def calculate_and_log_attention_stats(
    attention_matrix, 
    input_ids, 
    tokenizer, 
    output_csv_path, 
    item_size=4,
    pad_token_id=0
):
    """
    è®¡ç®—å¹¶è®°å½•æ³¨æ„åŠ›ç»Ÿè®¡ï¼ˆæœ€ç»ˆä¿®æ­£ç‰ˆï¼Œå·²é€‚é…å·¦paddingï¼Œå¸¦è°ƒè¯•ä¿¡æ¯ï¼‰ã€‚
    """
    print(f"   [è°ƒè¯•] æ¥æ”¶åˆ° (å·¦padding) input_ids (å‰30ä¸ª): {input_ids[:30]}")

    # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šé€‚é…å·¦padding ---
    # ä»å¤´å¼€å§‹éå†ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªâ€œéâ€padding tokençš„ç´¢å¼•ï¼Œè¿™æ‰æ˜¯æœ‰æ•ˆåºåˆ—çš„å¼€å§‹
    valid_start_index = -1
    for i, token_id in enumerate(input_ids):
        if token_id != pad_token_id:
            valid_start_index = i
            break
    
    # --- è°ƒè¯•ï¼šæ‰“å°è®¡ç®—å‡ºçš„åºåˆ—ä¿¡æ¯ ---
    print(f"   [è°ƒè¯•] ç¬¬ä¸€ä¸ªæœ‰æ•ˆtokençš„ç´¢å¼• (valid_start_index): {valid_start_index}")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆtokenï¼ˆæ•´ä¸ªåºåˆ—éƒ½æ˜¯paddingï¼‰ï¼Œæˆ–åªæœ‰ä¸€ä¸ªtoken
    if valid_start_index == -1 or (len(input_ids) - valid_start_index) <= 1:
        real_seq_len = 0 if valid_start_index == -1 else (len(input_ids) - valid_start_index)
        print(f"   [è°ƒè¯•] çœŸå®åºåˆ—é•¿åº¦ <= 1 (é•¿åº¦ä¸º {real_seq_len})ï¼Œæ— æ³•è¿›è¡Œitemåˆ†æï¼Œå‡½æ•°æå‰è¿”å›(0,0,0)ã€‚")
        with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['error'])
            writer.writerow([f'No valid tokens to analyze. Real sequence length was {real_seq_len}.'])
        return 0, 0, 0

    # æ ¹æ®æœ‰æ•ˆåºåˆ—çš„èµ·ç‚¹ï¼Œå¯¹æ•°æ®è¿›è¡Œåˆ‡ç‰‡
    valid_input_ids = input_ids[valid_start_index:]
    real_seq_len = len(valid_input_ids)
    
    if hasattr(attention_matrix, 'cpu'):
        attention_matrix_np = attention_matrix.cpu().numpy()
    else:
        attention_matrix_np = attention_matrix
        
    # æ³¨æ„åŠ›çŸ©é˜µä¹Ÿè¦è¿›è¡ŒåŒæ ·çš„åˆ‡ç‰‡
    valid_attention_matrix = attention_matrix_np[valid_start_index:, valid_start_index:]
    
    print(f"   [è°ƒè¯•] åˆ‡ç‰‡åçš„çœŸå®åºåˆ—é•¿åº¦: {real_seq_len}")
    print(f"   [è°ƒè¯•] åˆ‡ç‰‡åçš„ attention_matrix å½¢çŠ¶: {valid_attention_matrix.shape}")

    token_stats = []
    
    # åç§»é‡ä»ç„¶æ˜¯1ï¼Œå› ä¸ºæˆ‘ä»¬è¦è·³è¿‡æœ‰æ•ˆåºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªtokenï¼ˆuser tokenï¼‰
    offset = 1

    # åœ¨â€œæœ‰æ•ˆåºåˆ—â€çš„èŒƒå›´å†…è¿›è¡Œéå†
    for query_idx in range(offset, real_seq_len):
        query_token_id = valid_input_ids[query_idx]
        query_token = query_token_id

        item_index = (query_idx - offset) // item_size
        item_start_idx = offset + item_index * item_size
        item_end_idx = min(item_start_idx + item_size, real_seq_len)

        own_item_scores_with_self = []
        own_item_scores_no_self = []
        other_item_scores = []

        for key_idx in range(offset, real_seq_len):
            score = valid_attention_matrix[query_idx, key_idx]

            if item_start_idx <= key_idx < item_end_idx:
                own_item_scores_with_self.append(score)
                if query_idx != key_idx:
                    own_item_scores_no_self.append(score)
            else:
                other_item_scores.append(score)
        
        avg_own_with_self = np.mean(own_item_scores_with_self) if own_item_scores_with_self else 0
        avg_own_no_self = np.mean(own_item_scores_no_self) if own_item_scores_no_self else 0
        avg_other = np.mean(other_item_scores) if other_item_scores else 0
        
        token_stats.append({
            # æ³¨æ„ï¼šè¿™é‡Œçš„ç´¢å¼•æ˜¯ç›¸å¯¹äºæœ‰æ•ˆåºåˆ—çš„ï¼Œè€Œä¸æ˜¯åŸå§‹åºåˆ—
            'token_index_in_valid_seq': query_idx,
            'token': query_token,
            'item_index': item_index,
            'avg_attention_to_own_item (with self)': f"{avg_own_with_self:.6f}",
            'avg_attention_to_own_item (no self)': f"{avg_own_no_self:.6f}",
            'avg_attention_to_other_items': f"{avg_other:.6f}"
        })

    if not token_stats:
        print("   [è°ƒè¯•] å¾ªç¯ç»“æŸä½†æœªæ”¶é›†åˆ°ä»»ä½•tokenç»Ÿè®¡æ•°æ®ï¼Œè¿”å›(0,0,0)ã€‚")
        return 0, 0, 0
            
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=token_stats[0].keys())
        writer.writeheader()
        writer.writerows(token_stats)
        
    total_avg_own_with_self = np.mean([float(s['avg_attention_to_own_item (with self)']) for s in token_stats])
    total_avg_own_no_self = np.mean([float(s['avg_attention_to_own_item (no self)']) for s in token_stats])
    total_avg_other = np.mean([float(s['avg_attention_to_other_items']) for s in token_stats])

    return total_avg_own_with_self, total_avg_own_no_self, total_avg_other

vis_config = {
    "shared_prefix_1": 5,
    "shared_prefix_2": 5,
    "shared_prefix_3": 5,
    "shared_token_no_prefix": 5,
    "other_samples": 5
}
def evaluate_model_with_constrained_beam_search(
    model,
    eval_dataloader,
    accelerator,
    tokenizer,
    k_list=[1, 5, 10],
    num_beams=10,
    max_gen_length=5,
    logger=None,
    mode="Evaluation",
    visualization_config=None,
    visualization_dir="attention_heatmaps",
    output_json_path="predictions.json",
    vector_output_path="vocab_vectors.txt",
):
    """
    è¯„ä¼°æ¨¡å‹æˆ–æ ¹æ®ç‰¹å®šæ ‡å‡†å¯è§†åŒ–æ³¨æ„åŠ›ã€‚
    - visualization_config (dict): æ¿€æ´»å¯è§†åŒ–æ¨¡å¼ï¼Œå¹¶æŒ‡å®šå„ç±»åˆ«çš„æ ·æœ¬æ•°é‡ã€‚
        ä¾‹å¦‚: {'shared_prefix_1': 5, 'shared_prefix_2': 5, 'shared_token_no_prefix': 3}
    """
    model.eval()

    # =================================================================================
    # å¯è§†åŒ–é€»è¾‘
    # =================================================================================
    if visualization_config and accelerator.is_main_process:
        print("è¿›å…¥æ¡ä»¶å¯è§†åŒ–æ¨¡å¼...")
        os.makedirs(visualization_dir, exist_ok=True)
        print(f"çƒ­åŠ›å›¾å°†åˆ†ç±»ä¿å­˜åˆ°: '{visualization_dir}/'")

        required_counts = visualization_config.copy()
        samples_to_visualize = {} 

        print("ç¬¬ä¸€æ­¥: éå†æ•°æ®é›†ä»¥ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ ·æœ¬ (åŸºäºå†å²è®°å½•å†…éƒ¨æ¯”è¾ƒ)...")
        # ç­›é€‰ pass
        for batch_idx, batch in enumerate(tqdm(eval_dataloader, desc="ç­›é€‰æ ·æœ¬")):
            if all(v == 0 for v in required_counts.values()):
                print("å·²ä¸ºæ‰€æœ‰ç±»åˆ«æ‰¾åˆ°è¶³å¤Ÿæ•°é‡çš„æ ·æœ¬ã€‚")
                break 

            input_ids_cpu = batch['input_ids']

            for i in range(input_ids_cpu.size(0)):
                if all(v == 0 for v in required_counts.values()):
                    break
                
                # +++++ å…¨æ–°çš„ç­›é€‰é€»è¾‘ +++++
                historical_items = _get_historical_items(input_ids_cpu[i], tokenizer)
                if len(historical_items) < 2:
                    continue

                pairs = list(combinations(historical_items, 2))

                # --- Pass 1: æ£€æŸ¥æ•´ä¸ªå†å²ä¸­æœ€é«˜çº§åˆ«çš„å‰ç¼€åŒ¹é… ---
                best_prefix_level = 0
                for item1, item2 in pairs:
                    if len(item1) >= 3 and len(item2) >= 3 and item1[:3] == item2[:3]:
                        best_prefix_level = max(best_prefix_level, 3)
                    elif len(item1) >= 2 and len(item2) >= 2 and item1[:2] == item2[:2]:
                        best_prefix_level = max(best_prefix_level, 2)
                    elif len(item1) >= 1 and len(item2) >= 1 and item1[:1] == item2[:1]:
                        best_prefix_level = max(best_prefix_level, 1)

                found_category = None
                if best_prefix_level == 3:
                    found_category = 'shared_prefix_3'
                elif best_prefix_level == 2:
                    found_category = 'shared_prefix_2'
                elif best_prefix_level == 1:
                    found_category = 'shared_prefix_1'
                else:
                    # --- Pass 2: ä»…å½“å†å²ä¸­æ²¡æœ‰ä»»ä½•å‰ç¼€åŒ¹é…æ—¶ï¼Œæ‰æ£€æŸ¥å…±äº«token ---
                    has_shared_token = False
                    for item1, item2 in pairs:
                        if not set(item1[:3]).isdisjoint(set(item2[:3])):
                            has_shared_token = True
                            break
                    if has_shared_token:
                        found_category = 'shared_token_no_prefix'
                if found_category is None:
                    found_category = 'other_samples'
                # å¦‚æœæ‰¾åˆ°äº†ç¬¦åˆæ¡ä»¶çš„ç±»åˆ«ï¼Œå¹¶ä¸”è¯¥ç±»åˆ«è¿˜éœ€è¦æ ·æœ¬ï¼Œåˆ™è®°å½•
                if found_category and required_counts.get(found_category, 0) > 0:
                    sample_key = (batch_idx, i)
                    if sample_key not in samples_to_visualize:
                        samples_to_visualize[sample_key] = found_category
                        required_counts[found_category] -= 1
                        print(f"  [æ‰¾åˆ°æ ·æœ¬] Batch {batch_idx}, Sample {i} -> åˆ†ç±»åˆ° '{found_category}'. è¯¥ç±»åˆ«è¿˜éœ€ {required_counts[found_category]} ä¸ª.")

        # --- å°†ç­›é€‰ç»“æœé‡ç»„ï¼Œæ–¹ä¾¿åç»­å¤„ç† ---
        batches_to_process = {}
        for (batch_idx, sample_idx), category in samples_to_visualize.items():
            if batch_idx not in batches_to_process:
                batches_to_process[batch_idx] = []
            batches_to_process[batch_idx].append((sample_idx, category))

        if not batches_to_process:
            print("æœªèƒ½æ‰¾åˆ°ä»»ä½•ç¬¦åˆå¯è§†åŒ–æ¡ä»¶çš„æ ·æœ¬ã€‚")
            return

        print(f"\nç­›é€‰å®Œæˆï¼å…±æ‰¾åˆ° {len(samples_to_visualize)} ä¸ªæ ·æœ¬ç”¨äºå¯è§†åŒ–ã€‚")
        print("ç¬¬äºŒæ­¥: å¼€å§‹å¯¹ç­›é€‰å‡ºçš„æ ·æœ¬è¿›è¡Œå¯è§†åŒ–...")

        # å¯è§†åŒ– pass (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜)
        device = accelerator.device
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(eval_dataloader, desc="ç”Ÿæˆçƒ­åŠ›å›¾")):
                if batch_idx not in batches_to_process:
                    continue

                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                outputs = model.generate(
                    input_ids=input_ids, attention_mask=attention_mask, max_length=2,
                    num_beams=1, return_dict_in_generate=True, output_attentions=True
                )
                if not (hasattr(outputs, 'encoder_attentions') and outputs.encoder_attentions is not None):
                    print("é”™è¯¯: æ¨¡å‹è¾“å‡ºä¸­æœªæ‰¾åˆ° 'encoder_attentions'ã€‚")
                    return
                
                last_layer_attentions = outputs.encoder_attentions[-1]
                
                for sample_idx, category in batches_to_process[batch_idx]:
                    print(f"\n--- æ­£åœ¨å¯è§†åŒ– Batch {batch_idx}, Sample {sample_idx} (ç±»åˆ«: {category}) ---")
                    
                    sample_attentions = last_layer_attentions[sample_idx]
                    sample_input_ids = input_ids[sample_idx].cpu().tolist()
                    
                    sample_output_dir = os.path.join(visualization_dir, category, f"batch_{batch_idx}_sample_{sample_idx}")
                    os.makedirs(sample_output_dir, exist_ok=True)
                    
                    avg_attention_weights = sample_attentions.mean(dim=0)
                    avg_file_path = os.path.join(sample_output_dir, "heatmap_average.png")
                    plot_encoder_attention_heatmap(avg_attention_weights, sample_input_ids, file_path=avg_file_path)
                    print(f"   [ç»Ÿè®¡] æ­£åœ¨ä¸ºæ ·æœ¬è®¡ç®—æ³¨æ„åŠ›ç»Ÿè®¡æ•°æ®...")
                    stats_csv_path = os.path.join(sample_output_dir, "attention_statistics.csv")
                    
                    # ä½¿ç”¨å¹³å‡æ³¨æ„åŠ›çŸ©é˜µè¿›è¡Œè®¡ç®—
                    total_avg_own, total_avg_own_no_self, total_avg_other = calculate_and_log_attention_stats(
                        attention_matrix=avg_attention_weights,
                        input_ids=sample_input_ids,
                        tokenizer=tokenizer,
                        output_csv_path=stats_csv_path,
                        item_size=4  # æ ¹æ®æ‚¨çš„å®šä¹‰ï¼Œæ¯ä¸ªitemæ˜¯4ä¸ªtoken
                    )
                    
                    print(f"   [ç»Ÿè®¡] è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {stats_csv_path}")
                    print(f"   [ç»Ÿè®¡] æ ·æœ¬æ€»ä½“å¹³å‡æ³¨æ„åŠ›:")
                    print(f"     - å¯¹æœ¬Item (å«è‡ªèº«): {total_avg_own:.6f}")
                    print(f"     - å¯¹æœ¬Item (ä¸å«è‡ªèº«): {total_avg_own_no_self:.6f}")
                    print(f"     - å¯¹å…¶ä»–Item: {total_avg_other:.6f}")
                    num_heads = sample_attentions.shape[0]
                    for head_idx in range(num_heads):
                        head_attention_weights = sample_attentions[head_idx]
                        file_path = os.path.join(sample_output_dir, f"head_{head_idx}_heatmap.png")
                        plot_encoder_attention_heatmap(
                            head_attention_weights,
                            sample_input_ids,
                            file_path=file_path,
                            head_index=head_idx
                        )
        
        print(f"\nå·²æˆåŠŸä¸º {len(samples_to_visualize)} ä¸ªç­›é€‰å‡ºçš„æ ·æœ¬ç”Ÿæˆå¯è§†åŒ–å›¾åƒã€‚")
        return

    if visualization_config:
         accelerator.wait_for_everyone() # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥
         if accelerator.is_main_process:
            print("å¯è§†åŒ–ä»»åŠ¡å·²åœ¨ä¸»è¿›ç¨‹å®Œæˆï¼Œè·³è¿‡è¯„ä¼°éƒ¨åˆ†ã€‚")
         return
         
    tokens_to_item_map = tokenizer.tokens2item
    item_to_tokens_map = tokenizer.item2tokens
    candidate_trie = Trie(tokenizer.item2tokens)
    prefix_allowed_fn = prefix_allowed_tokens_fn(candidate_trie)
    all_predictions = []
    all_labels = []
    device = accelerator.device
    progress_bar = tqdm(eval_dataloader, desc=f"{mode}", disable=not accelerator.is_main_process)
    for batch_idx, batch in enumerate(progress_bar):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        true_item_ids = batch['label_id']
        encoder_item_ids = batch['encoder_item_ids'].to(device) 
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_gen_length,
            encoder_item_ids=encoder_item_ids,
            num_beams=num_beams,
            num_return_sequences=num_beams,
            early_stopping=True,
            pad_token_id=0,
            eos_token_id=1,
            output_scores=True,
            return_dict_in_generate=True,
            prefix_allowed_tokens_fn=prefix_allowed_fn
        )
        generated_ids = outputs.sequences
        sequences_scores = outputs.sequences_scores
        batch_size = input_ids.size(0)
        generated_ids_reshaped = generated_ids.view(batch_size, num_beams, -1)[:, :, 1:]
        probabilities_reshaped = torch.exp(sequences_scores).view(batch_size, num_beams)
        batch_predictions = []
        batch_labels = []
        for i in range(batch_size):
            true_item_id = true_item_ids[i]
            true_item_tokens = item_to_tokens_map.get(true_item_id, []) 
            batch_labels.append({
                'id': true_item_id,
                'tokens': true_item_tokens
            })
            user_sequences = generated_ids_reshaped[i]
            user_probs = probabilities_reshaped[i]
            sorted_indices = torch.argsort(user_probs, descending=True)
            sorted_sequences = user_sequences[sorted_indices]
            predictions_with_tokens = []
            for seq in sorted_sequences:
                pred_tokens = seq.tolist()
                item_id = tokens_to_item_id(pred_tokens, tokens_to_item_map)
                predictions_with_tokens.append({
                    'id': item_id,
                    'tokens': pred_tokens
                })
            unique_predictions = []
            seen_ids = set()
            for pred in predictions_with_tokens:
                if pred['id'] not in seen_ids:
                    unique_predictions.append(pred)
                    seen_ids.add(pred['id'])
            batch_predictions.append(unique_predictions)
        gathered_predictions = accelerator.gather_for_metrics(batch_predictions)
        gathered_labels = accelerator.gather_for_metrics(batch_labels)
        all_predictions.extend(gathered_predictions)
        all_labels.extend(gathered_labels)

    if accelerator.is_main_process:
        metrics = {f'hit@{k}': 0 for k in k_list}
        metrics.update({f'ndcg@{k}': 0 for k in k_list})
        total_samples = len(all_labels)
        results_to_save = []
        for label_info, prediction_list in zip(all_labels, all_predictions):
            
            true_item_id = label_info['id']
            predicted_item_ids = [p['id'] for p in prediction_list]

            for k in k_list:
                top_k_ids = predicted_item_ids[:k]
                hit = 1 if true_item_id in top_k_ids else 0
                metrics[f'hit@{k}'] += hit
                if hit:
                    rank = top_k_ids.index(true_item_id) + 1
                    ndcg = 1 / math.log2(rank + 1)
                    metrics[f'ndcg@{k}'] += ndcg
            
            sample_result = {
                'label_id': label_info['id'],
                'label_tokens': label_info['tokens'],
                'predictions': [
                    {
                        'predicted_id': p['id'],
                        'predicted_tokens': p['tokens']
                    } for p in prediction_list
                ]
            }
            results_to_save.append(sample_result)
        final_metrics = {}
        for key, value in metrics.items():
            final_metrics[key] = value / total_samples

        if logger:
            logger.info(f"\n{mode} ç»“æœ (å…± {total_samples} ä¸ªæ ·æœ¬):")
            for k in k_list:
                logger.info(f"Hit@{k}: {final_metrics[f'hit@{k}']:.4f}, NDCG@{k}: {final_metrics[f'ndcg@{k}']:.4f}")
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, indent=4, ensure_ascii=False)
        
        if logger:
            logger.info(f"Top-10 é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_json_path}")
        return final_metrics
