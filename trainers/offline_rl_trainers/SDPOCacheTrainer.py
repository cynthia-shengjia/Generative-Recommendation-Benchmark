import warnings  
from collections import defaultdict  
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union  
import torch  
import torch.nn as nn  
import torch.nn.functional as F  
from datasets import Dataset  
from transformers import DataCollator, PreTrainedModel, PreTrainedTokenizerBase, Trainer, TrainingArguments  
from transformers.trainer_callback import TrainerCallback  
from transformers.modeling_outputs import BaseModelOutput

class SDPOTrainer(Trainer):  
    """S-DPO Trainer for Encoder-Decoder models"""  
      
    def __init__(
        self,
        model: Union[PreTrainedModel, nn.Module] = None,
        ref_model: Union[PreTrainedModel, nn.Module] = None,
        beta: float = 0.1,
        args: TrainingArguments = None,
        data_collator: Optional[DataCollator] = None,
        eval_data_collator: Optional[DataCollator] = None,
        label_pad_token_id: int = -100,
        padding_value: int = 0,
        train_dataset: Optional[Dataset] = None,
        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        model_init: Optional[Callable[[], PreTrainedModel]] = None,
        callbacks: Optional[List[TrainerCallback]] = None,
        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
        # ğŸ”´ æ–°å¢ï¼šç”¨äº evaluation çš„å‚æ•°
        compute_metrics: Optional[Callable] = None,
        generation_params: Optional[Dict] = None,
        item2tokens: Optional[Dict] = None,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
        **kwargs,
    ):
        self.label_pad_token_id = label_pad_token_id
        self.padding_value = padding_value
        self.beta = beta
        self.ref_model = ref_model
        self._stored_metrics = defaultdict(lambda: defaultdict(list))
        self.eval_data_collator = eval_data_collator
        
        # ğŸ”´ ä¿å­˜ evaluation ç›¸å…³å‚æ•°
        self._compute_metrics = compute_metrics
        self.generation_params = generation_params or {}
        self.item2tokens = item2tokens
        self.pad_token_id = pad_token_id
        self.eos_token_id = eos_token_id
        
        super().__init__(
            model=model,
            args=args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            model_init=model_init,
            compute_metrics=None,  # æˆ‘ä»¬è‡ªå·±å¤„ç†
            callbacks=callbacks,
            optimizers=optimizers,
            preprocess_logits_for_metrics=preprocess_logits_for_metrics,
        )
        
        if hasattr(self, "accelerator"):
            self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)
        else:
            raise AttributeError("Trainer does not have an accelerator object")
    
    def concatenated_forward(  
        self,  
        model: nn.Module,  
        batch: Dict[str, torch.Tensor]  
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:  
        """  
        ä¼˜åŒ–ç‰ˆ Encoder-Decoder å‰å‘ä¼ æ’­ï¼šEncoder åªè®¡ç®—ä¸€æ¬¡
          
        è¿”å›:  
            chosen_logps: [B]  
            rejected_logps: [B, N]  
            chosen_logits: [B, L, V]  
            rejected_logits: [B, N, L, V]  
        """  
        batch_size = batch["input_ids"].shape[0]  
        num_rejected = batch["rejected_labels"].shape[1]  
          
        # ===== 1. Encoder åªè®¡ç®—ä¸€æ¬¡ =====
        encoder_outputs = model.encoder(  
            input_ids=batch["input_ids"],  
            attention_mask=batch["attention_mask"],  
            return_dict=True,  
        )  
        # encoder_outputs.last_hidden_state: [B, seq_len, hidden_dim]
        
        # ===== 2. Decoder for Chosen =====  
        chosen_outputs = model(  
            encoder_outputs=encoder_outputs,  
            attention_mask=batch["attention_mask"],  
            labels=batch["chosen_labels"],  
        )  
        chosen_logits = chosen_outputs.logits.to(torch.float32)  # [B, L, V]  
          
        # è®¡ç®— chosen çš„ log probabilities  
        chosen_logps = self._get_batch_logps(  
            chosen_logits,  
            batch["chosen_labels"],  
            average_log_prob=False,  
        )  # [B]  
          
        # ===== 3. Decoder for Rejected (å¤ç”¨ Encoder è¾“å‡º) =====  
        # rejected_labels: [B, N, L] -> [B*N, L]  
        rejected_labels_flat = batch["rejected_labels"].view(batch_size * num_rejected, -1)  
          
        # å¤åˆ¶ encoder hidden states N æ¬¡
        # [B, seq_len, hidden_dim] -> [B*N, seq_len, hidden_dim]
        encoder_hidden_states_repeated = encoder_outputs.last_hidden_state.repeat_interleave(  
            num_rejected, dim=0  
        )  
        
        # å¤åˆ¶ attention_mask N æ¬¡
        attention_mask_repeated = batch["attention_mask"].repeat_interleave(num_rejected, dim=0)  
          
        # åˆ›å»ºæ–°çš„ encoder_outputs å¯¹è±¡
        repeated_encoder_outputs = BaseModelOutput(  
            last_hidden_state=encoder_hidden_states_repeated,  
        )  
        
        # Decoder å‰å‘ä¼ æ’­ï¼ˆä¸ä¼šé‡æ–°è®¡ç®— Encoderï¼‰
        rejected_outputs = model(  
            encoder_outputs=repeated_encoder_outputs,  
            attention_mask=attention_mask_repeated,  
            labels=rejected_labels_flat,  
        )  
        rejected_logits_flat = rejected_outputs.logits.to(torch.float32)  # [B*N, L, V]  
          
        # è®¡ç®— rejected çš„ log probabilities  
        rejected_logps_flat = self._get_batch_logps(  
            rejected_logits_flat,  
            rejected_labels_flat,  
            average_log_prob=False,  
        )  # [B*N]  
          
        # Reshape: [B*N] -> [B, N]  
        rejected_logps = rejected_logps_flat.view(batch_size, num_rejected)  
          
        # Reshape logits: [B*N, L, V] -> [B, N, L, V]  
        rejected_logits = rejected_logits_flat.view(  
            batch_size, num_rejected, rejected_logits_flat.shape[1], rejected_logits_flat.shape[2]  
        )  
          
        return chosen_logps, rejected_logps, chosen_logits, rejected_logits  

    def dpo_loss(    
        self,    
        policy_chosen_logps: torch.FloatTensor,      # [B]    
        policy_rejected_logps: torch.FloatTensor,    # [B, N]    
        reference_chosen_logps: torch.FloatTensor,   # [B]    
        reference_rejected_logps: torch.FloatTensor, # [B, N]    
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:    
        """    
        S-DPO Loss: åŒæ—¶å¯¹æ¯” chosen å’Œå¤šä¸ª rejected    
            
        Loss = -log Ïƒ(-log Î£_i exp(Î² * (r_rejected_i - r_chosen)))    
            = -log Ïƒ(-logsumexp(Î² * (r_rejected_i - r_chosen)))
            
        Args:    
            policy_chosen_logps: [B]    
            policy_rejected_logps: [B, N]    
            reference_chosen_logps: [B]    
            reference_rejected_logps: [B, N]    
            
        Returns:    
            losses: [B]    
            chosen_rewards: [B]    
            rejected_rewards: [B, N]    
        """    
        # è®¡ç®— log-ratios    
        chosen_logratios = policy_chosen_logps - reference_chosen_logps  # [B]    
        rejected_logratios = policy_rejected_logps - reference_rejected_logps  # [B, N]    
            
        # S-DPO æ ¸å¿ƒï¼šå¯¹æ‰€æœ‰ rejected åš softmax    
        # chosen_logratios: [B] -> [B, 1] for broadcasting    
        chosen_logratios_expanded = chosen_logratios.unsqueeze(1)  # [B, 1]    
            
        # logit_diff = Î² * (r_rejected_i - r_chosen): [B, N]
        logit_diff = self.beta * (rejected_logratios - chosen_logratios_expanded)  # [B, N]    
            
        # ä½¿ç”¨ logsumexp ä»£æ›¿ log(sum(exp(...)))ï¼Œæ•°å€¼æ›´ç¨³å®š
        # logsumexp(Î² * (r_rejected_i - r_chosen)) over N dimension: [B, N] -> [B]
        log_sum_exp = torch.logsumexp(logit_diff, dim=1)  # [B]
            
        # Loss = -log Ïƒ(-logsumexp(...))
        #      = -log(1 / (1 + exp(logsumexp(...))))
        #      = log(1 + exp(logsumexp(...)))
        #      = softplus(logsumexp(...))
        losses = -F.logsigmoid(-log_sum_exp)  # [B]
            
        # è®¡ç®— rewardsï¼ˆç”¨äº loggingï¼‰    
        chosen_rewards = self.beta * chosen_logratios.detach()  # [B]    
        rejected_rewards = self.beta * rejected_logratios.detach()  # [B, N]    
            
        return losses, chosen_rewards, rejected_rewards

    def get_batch_metrics(  
        self,  
        model,  
        batch: Dict[str, torch.Tensor],  
        train_eval: Literal["train", "eval"] = "train",  
    ):  
        """è®¡ç®— batch çš„ loss å’Œ metrics"""  
        metrics = {}  
          
        # Policy model å‰å‘ä¼ æ’­  
        (  
            policy_chosen_logps,      # [B]  
            policy_rejected_logps,    # [B, N]  
            policy_chosen_logits,     # [B, L, V]  
            policy_rejected_logits,   # [B, N, L, V]  
        ) = self.concatenated_forward(model, batch)  
          
        # Reference model å‰å‘ä¼ æ’­  
        with torch.no_grad():  
            (  
                reference_chosen_logps,    # [B]  
                reference_rejected_logps,  # [B, N]  
                _,  
                _,  
            ) = self.concatenated_forward(self.ref_model, batch)  
          
        # è®¡ç®— loss  
        losses, chosen_rewards, rejected_rewards = self.dpo_loss(  
            policy_chosen_logps,  
            policy_rejected_logps,  
            reference_chosen_logps,  
            reference_rejected_logps,  
        )  
          
        # è®¡ç®— accuracy: chosen æ˜¯å¦æ¯”æ‰€æœ‰ rejected éƒ½å¥½  
        reward_comparisons = chosen_rewards.unsqueeze(1) > rejected_rewards  # [B, N]  
        reward_accuracies = reward_comparisons.all(dim=1).float()  # [B]  
          
        # Logging metrics  
        prefix = "eval_" if train_eval == "eval" else ""  
          
        metrics[f"{prefix}rewards/chosen"] = chosen_rewards.cpu().numpy().mean()  
        metrics[f"{prefix}rewards/rejected_mean"] = rejected_rewards.cpu().numpy().mean()  
          
        # æ¯ä¸ª rejected çš„å¹³å‡ reward  
        num_rejected = rejected_rewards.shape[1]  
        for i in range(num_rejected):  
            metrics[f"{prefix}rewards/rejected{i+1}"] = rejected_rewards[:, i].cpu().numpy().mean()  
          
        metrics[f"{prefix}rewards/accuracies"] = reward_accuracies.cpu().numpy().mean()  
          
        # Margins: chosen - rejected  
        margins = chosen_rewards.unsqueeze(1) - rejected_rewards  # [B, N]  
        metrics[f"{prefix}rewards/margins_mean"] = margins.cpu().numpy().mean()  
        for i in range(num_rejected):  
            metrics[f"{prefix}rewards/margins_rejected{i+1}"] = margins[:, i].cpu().numpy().mean()  
          
        metrics[f"{prefix}logps/chosen"] = policy_chosen_logps.detach().cpu().numpy().mean()  
        metrics[f"{prefix}logps/rejected_mean"] = policy_rejected_logps.detach().cpu().numpy().mean()  
          
        metrics[f"{prefix}logits/chosen"] = policy_chosen_logits.detach().cpu().numpy().mean()  
        metrics[f"{prefix}logits/rejected_mean"] = policy_rejected_logits.detach().cpu().numpy().mean()  
          
        return losses.mean(), metrics  

    def _get_batch_logps(  
        self,  
        logits: torch.FloatTensor,  
        labels: torch.LongTensor,  
        average_log_prob: bool = False,  
    ) -> torch.FloatTensor:  
        """  
        è®¡ç®—ç»™å®š labels åœ¨ logits ä¸‹çš„ log probabilities  
          
        Args:  
            logits: [B, L, V]  
            labels: [B, L]  
          
        Returns:  
            log_probs: [B]  
        """  
        if logits.shape[:-1] != labels.shape:  
            raise ValueError("Logits and labels must have the same shape (except last dim)")  
          
        # Shift: é¢„æµ‹ä¸‹ä¸€ä¸ª token  
        labels = labels[:, 1:].clone()  
        logits = logits[:, :-1, :]  
          
        # Mask: å¿½ç•¥ label_pad_token_id  
        loss_mask = labels != self.label_pad_token_id  
          
        # å°† pad token æ›¿æ¢ä¸º 0ï¼ˆé¿å…ç´¢å¼•é”™è¯¯ï¼‰  
        labels[labels == self.label_pad_token_id] = 0  
          
        # è®¡ç®—æ¯ä¸ª token çš„ log probability  
        per_token_logps = torch.gather(  
            logits.log_softmax(-1),  
            dim=2,  
            index=labels.unsqueeze(2)  
        ).squeeze(2)  
          
        if average_log_prob:  
            return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)  
        else:  
            return (per_token_logps * loss_mask).sum(-1)  


    def compute_loss(
        self,
        model: Union[PreTrainedModel, nn.Module],
        inputs: Dict[str, Union[torch.Tensor, Any]],
        return_outputs=False,
        num_items_in_batch=None,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:
        """è®­ç»ƒæ—¶è°ƒç”¨ - ä½¿ç”¨ S-DPO loss"""
        loss, metrics = self.get_batch_metrics(model, inputs, train_eval="train")
        
        if self.accelerator.is_main_process:
            self.store_metrics(metrics, train_eval="train")
        
        if return_outputs:
            return (loss, metrics)
        return loss
    
    def prediction_step(  
        self,  
        model: Union[PreTrainedModel, nn.Module],  
        inputs: Dict[str, Union[torch.Tensor, Any]],  
        prediction_loss_only: bool,  
        ignore_keys: Optional[List[str]] = None,  
    ):  
        """è¯„ä¼°æ—¶è°ƒç”¨"""  
        if ignore_keys is None:  
            if hasattr(model, "config"):  
                ignore_keys = getattr(model.config, "keys_to_ignore_at_inference", [])  
            else:  
                ignore_keys = []  
          
        with torch.no_grad():  
            loss, metrics = self.get_batch_metrics(model, inputs, train_eval="eval")  
          
        # Log metrics  
        if self.accelerator.is_main_process:  
            self.store_metrics(metrics, train_eval="eval")  
          
        if prediction_loss_only:  
            return (loss.detach(), None, None)  
          
        # è¿”å› logits å’Œ labelsï¼ˆç”¨äºè®¡ç®—å…¶ä»–æŒ‡æ ‡ï¼‰  
        logits_dict = {  
            "eval_logits/chosen": metrics["eval_logits/chosen"],  
        }  
        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)  
        logits = torch.stack(logits).mean(axis=1) if logits else torch.tensor([0.0])  
        labels = torch.zeros(logits.shape[0])  
          
        return (loss.detach(), logits, labels)  

    def store_metrics(  
        self,  
        metrics: Dict[str, float],  
        train_eval: Literal["train", "eval"] = "train"  
    ) -> None:  
        """å­˜å‚¨ metrics"""  
        for key, value in metrics.items():  
            self._stored_metrics[train_eval][key].append(value)  

    def log(self, logs: Dict[str, float], *args, **kwargs) -> None:
        """
        Log metrics to the various objects watching training.
        
        Args:
            logs: Dictionary of metrics to log
            *args, **kwargs: Additional arguments passed to parent's log method
        """
        train_eval = "train" if "loss" in logs else "eval"
        
        # Add stored metrics to logs
        if train_eval in self._stored_metrics:
            for key, metrics in self._stored_metrics[train_eval].items():
                if len(metrics) > 0:
                    logs[key] = torch.tensor(metrics).mean().item()
            
            # Clear stored metrics for this phase
            self._stored_metrics[train_eval].clear()
        
        # Call parent's log method
        return super().log(logs, *args, **kwargs)
    
    def get_eval_dataloader(self, eval_dataset=None):
        """é‡å†™ä»¥ä½¿ç”¨ä¸åŒçš„ collator"""
        if eval_dataset is None:
            eval_dataset = self.eval_dataset
        
        # ä¸´æ—¶æ›¿æ¢ collator
        original_collator = self.data_collator
        if self.eval_data_collator is not None:
            self.data_collator = self.eval_data_collator
        
        dataloader = super().get_eval_dataloader(eval_dataset)
        
        # æ¢å¤åŸ collator
        self.data_collator = original_collator
        
        return dataloader

    

    def prediction_step(
        self,
        model: Union[PreTrainedModel, nn.Module],
        inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool,
        ignore_keys: Optional[List[str]] = None,
    ):
        """
        è¯„ä¼°æ—¶è°ƒç”¨ - ä½¿ç”¨æ ‡å‡†çš„ç”Ÿæˆè¯„ä¼°
        
        ğŸ”´ å…³é”®ä¿®æ”¹ï¼šä¸å†è®¡ç®— S-DPO lossï¼Œè€Œæ˜¯è¿›è¡Œç”Ÿæˆè¯„ä¼°
        """
        if ignore_keys is None:
            if hasattr(model, "config"):
                ignore_keys = getattr(model.config, "keys_to_ignore_at_inference", [])
            else:
                ignore_keys = []
        
        # ğŸ”´ æ£€æŸ¥æ˜¯å¦æœ‰ rejected_labelsï¼ˆè®­ç»ƒæ•°æ®æ ¼å¼ï¼‰
        has_rejected = "rejected_labels" in inputs
        
        if has_rejected:
            # å¦‚æœæœ‰ rejected_labelsï¼Œä½¿ç”¨ S-DPO è¯„ä¼°
            with torch.no_grad():
                loss, metrics = self.get_batch_metrics(model, inputs, train_eval="eval")
            
            if self.accelerator.is_main_process:
                self.store_metrics(metrics, train_eval="eval")
            
            if prediction_loss_only:
                return (loss.detach(), None, None)
            
            logits_dict = {
                "eval_logits/chosen": metrics["eval_logits/chosen"],
            }
            logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)
            logits = torch.stack(logits).mean(axis=1) if logits else torch.tensor([0.0])
            labels = torch.zeros(logits.shape[0])
            
            return (loss.detach(), logits, labels)
        
        else:
            # ğŸ”´ æ ‡å‡†çš„ç”Ÿæˆè¯„ä¼°ï¼ˆç”¨äº validation/testï¼‰
            with torch.no_grad():
                # ä½¿ç”¨ beam search ç”Ÿæˆ
                generated_ids = model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_length=self.generation_params.get('max_gen_length', 5),
                    num_beams=self.generation_params.get('num_beams', 10),
                    num_return_sequences=self.generation_params.get('num_beams', 10),
                    pad_token_id=self.pad_token_id,
                    eos_token_id=self.eos_token_id,
                    early_stopping=True,
                )
                
                # Reshape: [B*num_beams, L] -> [B, num_beams, L]
                batch_size = inputs["input_ids"].shape[0]
                num_beams = self.generation_params.get('num_beams', 10)
                generated_ids = generated_ids.view(batch_size, num_beams, -1)
            
            # å¦‚æœåªéœ€è¦ loss
            if prediction_loss_only:
                return (torch.tensor(0.0), None, None)
            
            # è¿”å›ç”Ÿæˆç»“æœå’Œæ ‡ç­¾ï¼ˆç”¨äº compute_metricsï¼‰
            labels = inputs.get("labels", inputs.get("decoder_input_ids"))
            
            return (torch.tensor(0.0), generated_ids, labels)
